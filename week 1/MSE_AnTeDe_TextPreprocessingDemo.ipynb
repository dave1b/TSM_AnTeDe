{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B76kYsvIF0b9"
   },
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logocompact/300x300/1613732714/logo-mse.png \"MSE Logo\") \n",
    "\n",
    "# AnTeDe Practical Work : Preprocessing Options\n",
    "\n",
    "by Fabian Märki (FHNW) and Andrei Popescu-Belis (HES-SO), modifications for Google Colab made by Daniel Perruchoud (FHNW)\n",
    "\n",
    "## Summary\n",
    "The aim of this notebook is to demonstrate three text preprocessing options, extending those seen in Lab 1 using NLTK.  There is no code to complete and no question to answer in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0f5nR5VHF0b_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675404187596,
     "user_tz": -60,
     "elapsed": 12258,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "93c4fdfb-c30b-4746-bafd-fde72e3d9ac7",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:32:59.812820Z",
     "start_time": "2024-02-26T16:32:54.181291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /Users/davebrunner/.local/share/virtualenvs/TSM_AnTeDe-bbc8cZgo/lib/python3.12/site-packages (0.1.73)\r\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /Users/davebrunner/.local/share/virtualenvs/TSM_AnTeDe-bbc8cZgo/lib/python3.12/site-packages (from contractions) (0.0.24)\r\n",
      "Requirement already satisfied: anyascii in /Users/davebrunner/.local/share/virtualenvs/TSM_AnTeDe-bbc8cZgo/lib/python3.12/site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\r\n",
      "Requirement already satisfied: pyahocorasick in /Users/davebrunner/.local/share/virtualenvs/TSM_AnTeDe-bbc8cZgo/lib/python3.12/site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Here are some of the packages that will be demonstrated.\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "!pip install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsENkaDSF0cA"
   },
   "source": [
    "## 1. Pre-processing with NLTK\n",
    "\n",
    "### a. Lemmatization with NLTK\n",
    "The purpose of lemmatization is to reduce different inflected forms of a word to a normalized one called _lemma_.  For example, a lemmatizer should be able to determine that _gone_, _going_ and _went_ all have the same lemma _go_.  The output of lemmatization is a proper word, so lemmatisation by simple suffix stripping (as with some stemming algorithms) is not sufficient.\n",
    "\n",
    "The goal of lemmatization is somehow similar to stemming (demonstrated below), as it maps several words into one common root, but the stem is not necessarily and actual word, while the lemma is.\n",
    "\n",
    "You will need to download the Punkt tokenizer and WordNet as well as Stopwords, by executing `nltk.download('punkt')`, `nltk.download('wordnet')` and `nltk.download('stopwords')`."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "id": "6VNsx87KICgj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675404222143,
     "user_tz": -60,
     "elapsed": 889,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "811b637c-220b-4b24-e7cb-ec80c48f4dde",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:02.291379Z",
     "start_time": "2024-02-26T16:32:59.813748Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/davebrunner/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ogfK9l_QF0cB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675404226242,
     "user_tz": -60,
     "elapsed": 244,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:02.307151Z",
     "start_time": "2024-02-26T16:33:02.291283Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() # from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMBp5cWFF0cC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675404324659,
     "user_tz": -60,
     "elapsed": 1962,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "630fb6e0-d8be-46ae-c797-e2e3bbbd625f",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:25.436490Z",
     "start_time": "2024-02-26T16:33:02.296878Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/davebrunner/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "['The',\n 'striped',\n 'bat',\n 'are',\n 'hanging',\n 'on',\n 'their',\n 'foot',\n 'for',\n 'rest',\n '.']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "[lemmatizer.lemmatize(w) for w in nltk.word_tokenize(\"The striped bats are hanging on their feet for rest.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOCutlMCF0cC"
   },
   "source": [
    "The success of lemmatization depends on indicating the correct part-of-speech of the word to the lemmatizer (as the second argument, named `pos`).  Part-of-speech tagging of words will be discussed in a later course, but for now you can download (once) a POS tagger for NLTK by running `nltk.download('averaged_perceptron_tagger')`. This tagger will label every word with its POS tag.  \n",
    "\n",
    "Then, the tags have to be converted into those known by WordNet (i.e. NOUN, ADJ, ADV, or VERB) so that the WordNetLemmatizer can operate. To convert them, we define the converter function `get_wordnet_pos`.  We then get the result of lemmatization, and can compare it with the previous one.  (See the [NLTK doc here](http://www.nltk.org/api/nltk.corpus.reader.html?highlight=nltk%20corpus%20wordnet#module-nltk.corpus.reader.wordnet), and a [SO question here](https://stackoverflow.com/questions/51634328/wordnetlemmatizer-different-handling-of-wn-adj-and-wn-adj-sat))."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ],
   "metadata": {
    "id": "IF_Wag4DIG3w",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675404396657,
     "user_tz": -60,
     "elapsed": 231,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "d4c20335-8f33-4baf-e79b-205cc206b552",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:26.809851Z",
     "start_time": "2024-02-26T16:33:25.435321Z"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "h5foE20xF0cD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675404424347,
     "user_tz": -60,
     "elapsed": 232,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:26.814747Z",
     "start_time": "2024-02-26T16:33:26.810824Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):    \n",
    "    if tag[0] == \"J\":\n",
    "        return wordnet.ADJ\n",
    "    elif tag[0] == \"N\":\n",
    "        return wordnet.NOUN\n",
    "    elif tag[0] == \"V\":\n",
    "        return wordnet.VERB\n",
    "    elif tag[0] == \"R\":\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WcQj_yr0F0cE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675404427886,
     "user_tz": -60,
     "elapsed": 243,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "a4ef13cd-441e-4d6b-d11a-2fe151dc2f03",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:26.866549Z",
     "start_time": "2024-02-26T16:33:26.814347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['The',\n 'striped',\n 'bat',\n 'be',\n 'hang',\n 'on',\n 'their',\n 'foot',\n 'for',\n 'rest',\n '.']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1])) \n",
    " for w in nltk.pos_tag(nltk.word_tokenize(\"The striped bats were hanging on their feet for rest.\"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRCJ95foF0cE"
   },
   "source": [
    "### b. Stemming with NLTK\n",
    "Stemming is the process of reducing a word into its stem, i.e. its _root form_, which is not necessarily a word by itself. For example, the words _fish_, _fishes_ and _fishing_ all stem into _fish_, which is a correct word. On the other side, the words _study_, _studies_ and _studying_ stem into _studi_, which is not an English word.\n",
    "\n",
    "Commonly, stemming algorithms (a.k.a. stemmers) are based on rules for suffix stripping.  The most famous example is the **Porter stemmer**, introduced in the 1980's and currently implemented in a variety of programming languages.  The **Snowball stemmer** is an improved version of the Porter stemmer.\n",
    "\n",
    "Traditionally, search engines and other IR applications have applied stemming to improve the chance of matching different forms of a word, treating them as interchangeable, which may or may not be appropriate when searching.\n",
    "\n",
    "Stemming can be seen as a quick and dirty method of chopping off words to their root forms, working especially on English.  Lemmatization is operation that requires more linguistic knowledge, such as dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxUiQfODF0cF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675404571985,
     "user_tz": -60,
     "elapsed": 271,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "cd7ae241-d2ec-448e-b7bc-e9c8eee68331",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:26.867498Z",
     "start_time": "2024-02-26T16:33:26.866054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "troubl troubl troubl\n",
      "happi happier happiest\n",
      "cat cat\n",
      "is are be\n"
     ]
    }
   ],
   "source": [
    "ls = SnowballStemmer(\"english\") # from NLTK\n",
    "print(ls.stem(\"trouble\"), ls.stem(\"troubling\"), ls.stem(\"troubled\"))\n",
    "print(ls.stem(\"happy\"), ls.stem(\"happier\"), ls.stem(\"happiest\"))\n",
    "print(ls.stem(\"cat\"), ls.stem(\"cats\"))\n",
    "print(ls.stem(\"is\"), ls.stem(\"are\"), ls.stem(\"be\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LL8z7z2oF0cG"
   },
   "source": [
    "## 2. Pre-processing with our own TextPreprocessor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Px444BvPF0cG"
   },
   "source": [
    "We defined our own TextPreprocessor class, compatible with the processing pipeline of the `scikit-learn` library.  This class is available in the file `TextPreprocessor.py` provided with the lab, which should be imported.  It is inspired by two documents available [here](https://towardsdatascience.com/text-preprocessing-steps-and-universal-pipeline-94233cb6725a) and [here](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html).  **You can use it in later AnTeDe labs.**\n",
    "\n",
    "For usage in Google Colab, place `TextPreprocessor.py` in the same folder as this notebook and mount your Google Drive to be able to access `TextPreprocessor.py` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from TextPreprocessor import *"
   ],
   "metadata": {
    "id": "tFUAp_BsQG7j",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675404720110,
     "user_tz": -60,
     "elapsed": 20717,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "89c235ce-b110-46c5-beff-10cc310815c4",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:54.327242Z",
     "start_time": "2024-02-26T16:33:54.310639Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAz1wD8mF0cG"
   },
   "source": [
    "We will use here a shortened version of the Lee Background Corpus [described here](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF). The shortened version consists of 300 documents selected from the Australian Broadcasting Corporation's news mail service. It consists of texts of headline stories from around the years 2000-2001.  It is available as test data in the `gensim` package, so you do not need to download it separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_znqD7_LF0cH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675405435747,
     "user_tz": -60,
     "elapsed": 251,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:54.339923Z",
     "start_time": "2024-02-26T16:33:54.330554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the documents into a Pandas data frame.  Code inspired from:\n",
    "# https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb\n",
    "\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "text = open(lee_train_file).read().splitlines()\n",
    "data_df = pd.DataFrame({'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data_df.head()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "eGI8FaJ1ttDX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675405439172,
     "user_tz": -60,
     "elapsed": 368,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "96e8a59a-62be-4279-f255-048778d92048",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:54.360960Z",
     "start_time": "2024-02-26T16:33:54.343280Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text\n0  Hundreds of people have been forced to vacate ...\n1  Indian security forces have shot dead eight su...\n2  The national road toll for the Christmas-New Y...\n3  Argentina's political and economic crisis has ...\n4  Six midwives have been suspended at Wollongong...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hundreds of people have been forced to vacate ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Indian security forces have shot dead eight su...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The national road toll for the Christmas-New Y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Argentina's political and economic crisis has ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Six midwives have been suspended at Wollongong...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yP3FXrdfF0cH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675405486789,
     "user_tz": -60,
     "elapsed": 5612,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "322c6636-25b6-4029-865f-b1216af99ebd",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:57.186433Z",
     "start_time": "2024-02-26T16:33:54.351237Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davebrunner/.local/share/virtualenvs/TSM_AnTeDe-bbc8cZgo/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 2.8307975839998107\n"
     ]
    }
   ],
   "source": [
    "# Run our TextPreprocessor and chronometer it.\n",
    "\n",
    "language = 'english'\n",
    "stop_words = set(stopwords.words(language)) # from NLTK: do nltk.download('stopwords') once.\n",
    "for sw in ['\\\"', '\\'', '\\'\\'', '`', '``', '\\'s']:\n",
    "    stop_words.add(sw)\n",
    "\n",
    "processor = TextPreprocessor( # these are only a few of the options of TextPreprocessor (see code for more)\n",
    "    language = language,\n",
    "    pos_tags = {wordnet.ADJ, wordnet.NOUN},\n",
    "    stopwords = stop_words,\n",
    ")\n",
    "start = timer()\n",
    "data_df['processed'] = processor.transform(data_df['text'])\n",
    "end = timer()\n",
    "print(\"Took: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MrXh-_5kF0cH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675405492370,
     "user_tz": -60,
     "elapsed": 236,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "8e72079f-6115-4732-b343-65ac98c0b7d1",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:57.187164Z",
     "start_time": "2024-02-26T16:33:57.185218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hundreds of people have been forced to vacate their homes in the Southern Highlands of New South Wales as strong winds today pushed a huge bushfire towards the town of Hill Top. A new blaze near Goulburn, south-west of Sydney, has forced the closure of the Hume Highway. At about 4:00pm AEDT, a marked deterioration in the weather as a storm cell moved east across the Blue Mountains forced authorities to make a decision to evacuate people from homes in outlying streets at Hill Top in the New South Wales southern highlands. An estimated 500 residents have left their homes for nearby Mittagong. The New South Wales Rural Fire Service says the weather conditions which caused the fire to burn in a finger formation have now eased and about 60 fire units in and around Hill Top are optimistic of defending all properties. As more than 100 blazes burn on New Year's Eve in New South Wales, fire crews have been called to new fire at Gunning, south of Goulburn. While few details are available at this stage, fire authorities says it has closed the Hume Highway in both directions. Meanwhile, a new fire in Sydney's west is no longer threatening properties in the Cranebrook area. Rain has fallen in some parts of the Illawarra, Sydney, the Hunter Valley and the north coast. But the Bureau of Meteorology's Claire Richards says the rain has done little to ease any of the hundred fires still burning across the state. \"The falls have been quite isolated in those areas and generally the falls have been less than about five millimetres,\" she said. \"In some places really not significant at all, less than a millimetre, so there hasn't been much relief as far as rain is concerned. \"In fact, they've probably hampered the efforts of the firefighters more because of the wind gusts that are associated with those thunderstorms.\"  \n",
      "\n",
      "hundred people vacate home southern highland new south wale strong wind today huge bushfire towards town hill top new blaze near goulburn south-west sydney closure hume highway 4:00pm aedt marked deterioration weather storm cell east across blue mountain authority decision evacuate people home street hill top new south wale southern highland resident left home mittagong new south wale rural fire service weather condition fire burn finger formation fire unit around hill top optimistic property blaze burn new year eve new south wale fire crew new fire south goulburn detail available stage fire authority hume highway direction new fire sydney west longer property cranebrook area rain part illawarra sydney hunter valley north coast bureau meteorology claire richards rain little ease fire burning across state fall isolated area fall five millimetre place significant millimetre much relief rain concerned fact effort firefighter wind gust thunderstorm\n"
     ]
    }
   ],
   "source": [
    "print(data_df['text'].iloc[0], '\\n')\n",
    "print(data_df['processed'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_P8pr8iF0cH"
   },
   "source": [
    "## 3. Pre-processing with the Gensim library\n",
    "[Gensim](https://radimrehurek.com/gensim/) is a Python library widely used for topic modeling.  It provides handy utilities to preprocess text, documented [here](https://radimrehurek.com/gensim/parsing/preprocessing.html) and [here](https://github.com/thunlp/topical_word_embeddings/blob/master/TWE-2/gensim/parsing/preprocessing.py).  A simple example is as follows (don't forget to `import gensim`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9a-ukhAtF0cI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675405521330,
     "user_tz": -60,
     "elapsed": 262,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "e3e1cae1-66c3-4a7f-e811-1741edb856cd",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:57.191294Z",
     "start_time": "2024-02-26T16:33:57.188532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[['hello',\n  'world',\n  'weather',\n  'todai',\n  'isn',\n  'tall',\n  'tall',\n  'isn',\n  'tall',\n  'appl',\n  'hand',\n  'isn',\n  'correct',\n  'titl',\n  'goe',\n  'bold',\n  'text',\n  'italic',\n  'text',\n  'stripe',\n  'bat',\n  'hang']]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_documents([\"\"\"\n",
    "<i>Hello</i> <b>World</b> 9!\", \"Th3     weather_is really g00d today, isn't it?\n",
    "I'm tall, you're tall, but he isn't tall. But he's an apple in his hand isn't correct.\n",
    "o ö ô e é è o ö a ä à n ñ\n",
    "<h1>Title Goes Here</h1> \n",
    "\n",
    "<b>Bolded Text</b>\n",
    "<i>Italicized Text</i>\n",
    "The striped bats are hanging.\n",
    "\"\"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqmlBvaeF0cI"
   },
   "source": [
    "It is also possible to use gensim's preprocessing utility on the text introduced above.  This does not perform lemmatization, but stemming (on English), and generates a list of words.  We can compare the timing and then the outputs on the first text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bEHsyYxPF0cI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675405713201,
     "user_tz": -60,
     "elapsed": 1034,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "b5e99f7c-b965-459d-9d27-c411e47106fc",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:57.274701Z",
     "start_time": "2024-02-26T16:33:57.239368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.07995054200000595\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "data_df['processed_gensim'] = preprocess_documents(data_df['text'])\n",
    "end = timer()\n",
    "print(\"Took: \"+str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1EQ1_HcZF0cI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1675405719796,
     "user_tz": -60,
     "elapsed": 234,
     "user": {
      "displayName": "Daniel Perruchoud",
      "userId": "15604693075568907580"
     }
    },
    "outputId": "ff0647a8-10f4-468e-c3fd-3cec1230d665",
    "ExecuteTime": {
     "end_time": "2024-02-26T16:33:57.275597Z",
     "start_time": "2024-02-26T16:33:57.273961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hundr', 'peopl', 'forc', 'vacat', 'home', 'southern', 'highland', 'new', 'south', 'wale', 'strong', 'wind', 'todai', 'push', 'huge', 'bushfir', 'town', 'hill', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'sydnei', 'forc', 'closur', 'hume', 'highwai', 'aedt', 'mark', 'deterior', 'weather', 'storm', 'cell', 'move', 'east', 'blue', 'mountain', 'forc', 'author', 'decis', 'evacu', 'peopl', 'home', 'outli', 'street', 'hill', 'new', 'south', 'wale', 'southern', 'highland', 'estim', 'resid', 'left', 'home', 'nearbi', 'mittagong', 'new', 'south', 'wale', 'rural', 'servic', 'sai', 'weather', 'condit', 'caus', 'burn', 'finger', 'format', 'eas', 'unit', 'hill', 'optimist', 'defend', 'properti', 'blaze', 'burn', 'new', 'year', 'ev', 'new', 'south', 'wale', 'crew', 'call', 'new', 'gun', 'south', 'goulburn', 'detail', 'avail', 'stage', 'author', 'sai', 'close', 'hume', 'highwai', 'direct', 'new', 'sydnei', 'west', 'longer', 'threaten', 'properti', 'cranebrook', 'area', 'rain', 'fallen', 'part', 'illawarra', 'sydnei', 'hunter', 'vallei', 'north', 'coast', 'bureau', 'meteorolog', 'clair', 'richard', 'sai', 'rain', 'littl', 'eas', 'fire', 'burn', 'state', 'fall', 'isol', 'area', 'gener', 'fall', 'millimetr', 'said', 'place', 'signific', 'millimetr', 'hasn', 'relief', 'far', 'rain', 'concern', 'fact', 'probabl', 'hamper', 'effort', 'firefight', 'wind', 'gust', 'associ', 'thunderstorm']\n"
     ]
    }
   ],
   "source": [
    "print(data_df['processed_gensim'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ctt988g1F0cJ"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "You are now aware of three ways of pre-processing texts, which will be useful in later labs:\n",
    "1. a set of NLTK functions;\n",
    "2. the in-house class `TextPreprocessing`;\n",
    "3. gensim's `preprocess_documents` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMvbcae-F0cJ"
   },
   "source": [
    "## Appendix\n",
    "Several modules may be helpful when preprocessing, but will not be demonstrated here.  If needed, they can be installed and used according to their documentation:\n",
    "* [Contraction](https://github.com/kootenpv/contractions) is a tool expanding English contractions (e.g., contractions.fix(\"I'm tall\") returns \"I am tall\") but installing it on Conda may be difficult due to some dependencies.\n",
    "   * _Tips for installing it:_ `conda install contractions` may not work, so better try `pip install contractions`.  The package has a small number of dependencies, but one of them ([`pyahocorasick`](https://github.com/WojciechMula/pyahocorasick)) may trigger C compilation errors upon installation with `pip` and will not work with `conda install` but should work with `conda install -c conda-forge pyahocorasick` (see Anaconda [documentation](https://anaconda.org/conda-forge/pyahocorasick)). \n",
    "* [Inflect](https://pypi.org/project/inflect/) is a library for manipulating English word inflections, which can generate plural or singular nouns, ordinals, and indefinite articles, and can convert numbers written in digits to words.\n",
    "* [Unicodedata](https://docs.python.org/3/library/unicodedata.html) provides access to the Unicode Character Database and may help to cleanup text with character conversion flaws, or convert characters with diacritics to ASCII equivalents (which may or may not be a good idea for languages like French or German)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
