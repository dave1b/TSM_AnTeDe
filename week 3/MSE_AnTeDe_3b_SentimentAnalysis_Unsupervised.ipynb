{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngmtgO_KGvCo"
   },
   "source": [
    "# AnTeDe Lab 3: Sentiment Analysis - Part B\n",
    "\n",
    "\n",
    "## Session goal\n",
    "The goal of this session is to build a baseline solution for unsupervised sentiment analysis. We begin by importing a well-known corpus of IMDB movie reviews labeled as either positive or negative. Take a few minutes to familiarize yourself with the variables *corpus* and *labels*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bimyiOZAGvCr",
    "outputId": "824b7227-5935-4af4-f2f9-a822ae6c8763",
    "ExecuteTime": {
     "end_time": "2024-03-14T07:38:56.214190Z",
     "start_time": "2024-03-14T07:38:04.109647Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import random, nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "corpus = [' '.join(movie_reviews.words(fileid)) \\\n",
    "          for category in movie_reviews.categories() \\\n",
    "          for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "labels = [category \\\n",
    "          for category in movie_reviews.categories() \\\n",
    "          for fileid in movie_reviews.fileids(category)]\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9nnzuAEGvCt"
   },
   "source": [
    "We extract a list of **positive_words** and a list of **negative_words** from the txt files in the **Words** folder. \n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "negative_words_url = 'https://ptrckprry.com/course/ssd/data/negative-words.txt'\n",
    "positive_words_url = 'https://ptrckprry.com/course/ssd/data/positive-words.txt'\n",
    "\n",
    "def download_from_url(url, filepath):\n",
    "  import requests\n",
    "  r = requests.get(url)\n",
    "  with open(filepath, 'wb') as f:\n",
    "      f.write(r.content)\n",
    "\n",
    "!mkdir Words\n",
    "download_from_url(positive_words_url, 'Words/positive-words.txt')\n",
    "download_from_url(negative_words_url, 'Words/negative-words.txt')"
   ],
   "metadata": {
    "id": "HbYTzTh9HOv5",
    "ExecuteTime": {
     "end_time": "2024-03-14T07:37:50.778102Z",
     "start_time": "2024-03-14T07:37:48.486956Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HKjDXu5zGvCu",
    "ExecuteTime": {
     "end_time": "2024-03-14T07:37:50.788674Z",
     "start_time": "2024-03-14T07:37:50.781066Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('Words/positive-words.txt', errors='ignore') as opened:\n",
    "    contents=opened.read()\n",
    "contents_lines=['a+'] + contents.split('a+')[1].split('\\n')\n",
    "\n",
    "positive_words = [x for x in contents_lines if len(x)>0]\n",
    "\n",
    "\n",
    "with open('Words/negative-words.txt', errors='ignore') as opened:\n",
    "    contents=opened.read()\n",
    "contents_lines=['2-faced'] + contents.split('2-faced')[1].split('\\n')\n",
    "\n",
    "negative_words = [x for x in contents_lines if len(x)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jqkDOPUGvCu"
   },
   "source": [
    "We reuse our helper function **get_metrics** from Lab 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9mPKYySgGvCv",
    "ExecuteTime": {
     "end_time": "2024-03-14T07:37:50.793379Z",
     "start_time": "2024-03-14T07:37:50.790148Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predicted_labels):\n",
    "        from sklearn import metrics\n",
    "        import numpy as np\n",
    "        print ('Accuracy:', np.round(\n",
    "            metrics.accuracy_score(true_labels,\n",
    "            predicted_labels), 3))\n",
    "        \n",
    "        from sklearn.metrics import classification_report\n",
    "        print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNs11NF5GvCv"
   },
   "source": [
    "Your task: complete the function **baseline_sentiment_analysis** so that:\n",
    "- it counts the number of positive words and the number of negative words in each review in the corpus;\n",
    "- if the positive word count is greater than the negative word count, it labels the review as positive\n",
    "- if the positive word count is lower than or equal to the negative word count, it labels the review as negative\n",
    "\n",
    "When you've completed the function, run the cell to measure the performance of your baseline unsupervised sentiment analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sf2AV8rbGvCw",
    "outputId": "0609e990-8fdc-4b75-8c3b-a12f9dbf47c4",
    "ExecuteTime": {
     "end_time": "2024-03-14T07:37:53.222760Z",
     "start_time": "2024-03-14T07:37:50.795506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.709\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.67      0.82      0.74      1000\n",
      "         pos       0.77      0.60      0.67      1000\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.72      0.71      0.71      2000\n",
      "weighted avg       0.72      0.71      0.71      2000\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def baseline_sentiment_analysis(corpus):\n",
    "    \n",
    "    labels=[]\n",
    "    scores=[]\n",
    "    \n",
    "    for document in corpus:\n",
    "        assert(1==1)\n",
    "        document_words = set(word for word in word_tokenize(document))\n",
    "        positive_words_in_doc = len(list(document_words.intersection(positive_words)))\n",
    "        negative_words_in_doc = len(list(document_words.intersection(negative_words)))\n",
    "        score=positive_words_in_doc-negative_words_in_doc\n",
    "        if score>0:\n",
    "            labels.append('pos')\n",
    "        else:\n",
    "            labels.append('neg')\n",
    "        \n",
    "        scores.append(score)    \n",
    "    return labels,  scores\n",
    "\n",
    "predicted_labels, scores = baseline_sentiment_analysis(corpus)\n",
    "\n",
    "try:\n",
    "    get_metrics(true_labels=labels,\n",
    "            predicted_labels=predicted_labels)\n",
    "except:\n",
    "    print ('The function baseline_sentiment_analysis needs your attention.')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# show som examples\n",
    "for i in range(5):\n",
    "    print(f\"It was predicted that, the document:\")\n",
    "    print(corpus[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
