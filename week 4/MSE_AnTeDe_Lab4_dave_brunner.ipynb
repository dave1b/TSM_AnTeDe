{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"J_rT74OE3u7X"},"source":["![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logo/0x150/1643104191/logo-mse.png)\n","\n","# AnTeDe Lab 4: Search Engine with the Vector Space Model\n","\n","## Summary\n","The aim of this lab is to build a simple document search engine based on TF-IDF document vectors. \n","\n","The lab is inspired by a notebook designed by [Kavita Ganesan](https://github.com/kavgan/nlp-in-practice/blob/master/TF-IDF/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb).\n","\n","<font color='green'>Please answer the questions in green within this notebook, and submit the completed notebook under the corresponding homework on Moodle.</font>"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269,"status":"ok","timestamp":1678792631556,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"1HfzVuMr3u7b","outputId":"55e38a84-e840-49f6-c05a-76e0405ad00e"},"outputs":[],"source":["import os    \n","import nltk  # on Colab, you mind find it helpful to run nltk.download('popular') to install packages\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger') \n","nltk.download('omw-1.4')\n","\n","import gensim\n","import pandas as pd\n","from nltk.corpus import stopwords, wordnet\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.metrics.pairwise import linear_kernel\n","from gensim import models, corpora, similarities"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3258,"status":"ok","timestamp":1678792636032,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"ng4zVE9O4Vsz","outputId":"7a960838-d941-4836-e735-5595c177fda7"},"outputs":[],"source":["!pip install contractions\n","import contractions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1985,"status":"ok","timestamp":1678792638008,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"cmSdbCvJ4H_e","outputId":"1aa170f5-4a8c-4b65-d75c-63f58335ef05"},"outputs":[{"data":{"text/plain":["' from google.colab import drive\\ndrive.mount(\\'/content/gdrive\\')\\n\\n# Modify path according to your configuration\\n# !ls \"/content/gdrive/MyDrive/ColabNotebooks/MSE_AnTeDe_Spring2022\"\\nimport sys\\nsys.path.insert(0,\\'/content/gdrive/MyDrive/ColabNotebooks/MSE_AnTeDe_Spring2022\\')\\n\\nfrom TextPreprocessor import * '"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["# Import TextProcessor.py from Google drive\n","# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","\n","# # Modify path according to your configuration\n","# # !ls \"/content/gdrive/MyDrive/ColabNotebooks/MSE_AnTeDe_Spring2022\"\n","# import sys\n","# sys.path.insert(0,'/content/gdrive/MyDrive/ColabNotebooks/MSE_AnTeDe_Spring2022')\n","\n","# from TextPreprocessor import * "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Import TextProcessor.py from local directory structure\n","import sys\n","module_path = os.path.abspath(os.path.join('..'))\n","if module_path not in sys.path:\n","    sys.path.append(module_path)\n","\n","from Lab1.TextPreprocessor import *\n"]},{"cell_type":"markdown","metadata":{"id":"S_-0tNus3u7d"},"source":["The data used in this lab is a set of 300 documents selected from the Australian Broadcasting Corporation's news mail service. It consists of texts of headline stories from around the years 2000-2001.  This is a shortened version of the Lee Background Corpus [described here](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF).  It is available as test data in the **gensim** package, so you do not need to download it separately.\n","\n","The following code will load the documents into a Pandas dataframe."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1678792638008,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"YPD9lzy93u7e"},"outputs":[],"source":["# Code inspired from:\n","# https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb\n","\n","test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n","lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n","text = open(lee_train_file).read().splitlines()\n","data_df = pd.DataFrame({'text': text})"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hJbDDf633u7f"},"source":["The following code will run our in-house Text Preprocessor provided in the `TextPreprocessor.py` file, and documented in the `MSE_AnTeDe_TextPreprocessingDemo.ipynb` notebook provided in Lab 1 (see Lab 1 archive on Moodle for both files).\n","\n","<font color='green'> **Question**: Please enhance the code by adding special characters such as e.g., \" ' ' \" as stopwords and uses adjective and noun POS tag sets in the TextPreprocessor function.</font>"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1678792638009,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"pUnbeSRb3u7g"},"outputs":[],"source":["language = 'english'\n","stop_words = set(stopwords.words(language))\n","# Extend the list here:\n","# BEGIN_REMOVE\n","# END_REMOVE\n","\n","processor = TextPreprocessor(\n","# Add options here:\n","# BEGIN_REMOVE    \n","# END_REMOVE \n",")"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":21824,"status":"ok","timestamp":1678792659830,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"2DAZdRyF3u7h"},"outputs":[],"source":["data_df['processed'] = processor.transform(data_df['text'])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can now look at a few examples of processed texts."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54,"status":"ok","timestamp":1678792659830,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"xS8OabTb3u7i","outputId":"ee87ed46-0d7b-47f0-92aa-d5b982a1dffd"},"outputs":[],"source":["print(data_df['processed'].iloc[136])"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":51,"status":"ok","timestamp":1678792659830,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"050vP9dMbMAL","outputId":"42dead0a-a9ee-4514-d0e5-0f78b0df7adf"},"outputs":[],"source":["data_df.head()"]},{"cell_type":"markdown","metadata":{"id":"CM1Fzwnn3u7j"},"source":["## Generation of document vectors with [Scikit-learn](https://scikit-learn.org/stable)\n","\n","We will use the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class from scikit-learn to create a vocabulary and generate word counts or *Term Frequencies* (TF).\n","    \n","The result is a  matrix representation of the counts: each column represents a _word_ in the vocabulary and each row represents a document in our dataset: the cell values are the word counts of the word in the document. \n","\n","The matrix is very sparse, because all words not appearing in a document have 0 counts.\n","\n","Recommended reading for usage and differences of scikit-learn’s Tfidftransformer and Tfidfvectorizer: \n","\n","https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/\n","\n","In order to start using TfidfTransformer you will first have to create a CountVectorizer to count the number of words (term frequency), limit your vocabulary size, apply stop words and etc."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":49,"status":"ok","timestamp":1678792659830,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"Rd5WqzEl3u7k"},"outputs":[],"source":["cv = CountVectorizer(max_features=3000) # keep only the 3000 most frequent words in the corpus\n","word_count_vector = cv.fit_transform(data_df['processed'])"]},{"cell_type":"markdown","metadata":{"id":"lgg6rP7k3u7l"},"source":["Let's look at some words from our vocabulary:"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":49,"status":"ok","timestamp":1678792659830,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"IfIC--5Y3u7l"},"outputs":[],"source":["feature_names = cv.get_feature_names_out()"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1678792659831,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"-U7E6PqE3u7l","outputId":"cbc4d19c-4b04-4169-e5c8-febdf634a05c"},"outputs":[],"source":["print(len(feature_names)) # has the max_features value been reached?\n","print(feature_names[2500:2505]) # try various slices\n","print(np.where(feature_names == 'hundred')[0]) # find a word's index\n","print(feature_names[1315]) # find a word corresponding to an index\n","print(cv.vocabulary_.items())\n","\n","# print(cv.vocabulary_.keys())\n","# print(cv.vocabulary_.values())"]},{"cell_type":"markdown","metadata":{"id":"VO-DTsR_YgFg"},"source":["Now, let’s check the shape of the term-document matrix, which should contain 300 documents from the Lee Corpus and 3000 terms. "]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45,"status":"ok","timestamp":1678792659831,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"jG0FYa-hYsQF","outputId":"8e0ad3ca-7d78-4b29-807a-aa0992e1ca85"},"outputs":[],"source":["word_count_vector.shape"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1678792659831,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"l8qwp0VXuUW4","outputId":"1c9a29a1-4e86-4480-9ba3-6d2dc099a059"},"outputs":[],"source":["# Output word counts for a particular word  \n","count_list = np.asarray(word_count_vector.sum(axis=0))[0]\n","word_count_dict = dict(zip(feature_names, count_list))\n","\n","res = {k: v for k, v in word_count_dict.items() if k.startswith('hundred')}\n","# res = {k: v for k, v in word_count_dict.items() if k.startswith('people')}\n","res"]},{"cell_type":"markdown","metadata":{"id":"3akOYQ1b3u7m"},"source":["**TfidfTransformer to Compute Inverse Document Frequency (IDF)**\n","\n","We now use the (sparse) matrix generated by `CountVectorizer` to compute the IDF values of each word.  Note that the IDF should in reality be based on a large and representative corpus."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1678792659831,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"h98ABx053u7n","outputId":"525d209b-0b7d-4cc3-91aa-762c3e43a5f4"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"],"text/plain":["TfidfTransformer()"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n","tfidf_transformer.fit(word_count_vector)"]},{"cell_type":"markdown","metadata":{"id":"g8EwZDDy3u7n"},"source":["The IDF values are stored in the `idf_` field of the `TfidfTransformer`.  It has the same size as the array of feature names (words)."]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1678792659831,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"bHTO_vsr3u7n","outputId":"222db547-8dce-4187-e89d-59645409ee05"},"outputs":[],"source":["print(len(tfidf_transformer.idf_)) # check length"]},{"cell_type":"markdown","metadata":{"id":"-x_pCo3OZSx0"},"source":["To get a glimpse of how the IDF values look, we are going to print it by placing the IDF values in a python DataFrame. The values will be sorted in ascending order."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":441},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1678792659831,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"_KgXfs-eZD1n","outputId":"4616c1aa-caed-4da4-8622-8da898d8e53e"},"outputs":[],"source":["# print a single idf value \n","print(tfidf_transformer.idf_[np.where(cv.get_feature_names_out() == 'hundred')]) # check IDF value of a word\n","\n","# print idf values in a data frame \n","df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(), columns=[\"idf_weights\"]) \n","# sort ascending \n","df_idf.sort_values(by=['idf_weights'])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QGDruZ243u7o"},"source":["**We define here two helper functions:**\n"," * the first one is a sorting function for the columns of a sparse matrix in COOrdinate format (a.k.a \"ijv\" or \"triplet\" format [explained here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html));\n"," * the second one extracts the feature names (*words*) and their TF-IDF values from the sorted list."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1678792659831,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"6o1XWhnO3u7o"},"outputs":[],"source":["def sort_coo(coo_matrix):\n","    tuples = zip(coo_matrix.col, coo_matrix.data)\n","    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n","\n","def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n","    \"\"\"get the feature names and TF-IDF score of top n items from sorted list\"\"\"\n","    \n","    #use only topn items from vector\n","    sorted_items = sorted_items[:topn]\n","\n","    score_vals = []\n","    feature_vals = []\n","\n","    for idx, score in sorted_items:\n","        fname = feature_names[idx]\n","        \n","        #keep track of feature name and its corresponding score\n","        score_vals.append(round(score, 3))\n","        feature_vals.append(feature_names[idx])\n","\n","    results= {}\n","    for idx in range(len(feature_vals)):\n","        results[feature_vals[idx]]=score_vals[idx]\n","    \n","    return results"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mv-AZ-v03u7o"},"source":["We now select a document for which we will generate TF-IDF values.  <font color=\"green\">Please select a random document of your choice between 0 and 300.</font>"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1678792659832,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"vCX0MvIC3u7o","outputId":"08716b8f-a4c1-44b3-829c-bc3daaad96b4"},"outputs":[],"source":["doc_orig = data_df['text'].iloc[136]\n","doc_processed = data_df['processed'].iloc[136]\n","print(doc_orig)\n","print(doc_processed)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Jq5v6Qwj3u7p"},"source":["The next instruction generates the vector of TF-IDF values for the document using the `tfidf_transformer`."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1678792659832,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"ymfc56jU3u7p"},"outputs":[],"source":["tf_idf_vector = tfidf_transformer.transform(cv.transform([doc_processed]))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SNu8SobW3u7p"},"source":["Next, we sort the words in the `tf_idf_vector` by decreasing TF-IDF values, first transforming the vector into a coordinate format ('coo'), and then applying our sorting function from above.  We then extract the words with the top 10 scores (and the scores) for the selected document using our second helper function from above and display them."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1678792659832,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"8FhBHurc3u7q","outputId":"84f56856-f886-49c0-a0b3-437a72af033f"},"outputs":[],"source":["sorted_items=sort_coo(tf_idf_vector.tocoo())\n","\n","topn_words = extract_topn_from_vector(feature_names, sorted_items, 10)\n","\n","print(doc_orig, '\\n', topn_words)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sauSc-w_cvtm"},"source":["Alternatively the TF-IDF values of the first document can be inspected by placing the TF-IDF scores from the first document into a pandas data frame."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1678792659832,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"GhF-b2f0cWsO","outputId":"01662749-6924-4dcf-fd8a-5788385088ab"},"outputs":[],"source":["feature_names = cv.get_feature_names_out() \n","#get tfidf vector for first document \n","first_document_vector=tf_idf_vector[0] \n","#densify and print the scores \n","df_firstdoc = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n","print(\"the size of the data frame is: \", df_firstdoc.shape)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1678792659832,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"BAIvkVDrf5WA","outputId":"9fee5ea6-b247-4e18-f861-e35451b8c5dd"},"outputs":[],"source":["df_firstdoc.sort_values(by=[\"tfidf\"],ascending=False).head(20)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1678792659832,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"J9cPBdJtf8ow","outputId":"f7e4adf1-f54d-4a1c-f442-000df2353a72"},"outputs":[],"source":["df_firstdoc.sort_values(by=[\"tfidf\"],ascending=False).tail(20)"]},{"cell_type":"markdown","metadata":{"id":"ffMVuiV2dBCV"},"source":["Notice that only certain words have scores. This is because our first document does not contain all of the top 3000 tokens which then show up as zeroes. Notice that the word “a” is missing from this list. This is possibly due to internal pre-processing of CountVectorizer where it removes single characters.\n","\n","The more common the word across documents, the lower its score and the more unique a word is to our first document the higher the score. So it’s working as expected except for the mysterious a that was chopped off."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4xBOrddm3u7q"},"source":["<font color=\"green\"> **Question**: Please comment briefly on the relevance of these words with respect to the document content.</font>"]},{"cell_type":"markdown","metadata":{"id":"pONBQI8F3u7q"},"source":["## Document-document similarity using scikit-learn"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JUquDZi53u7q"},"source":["In this section, you will write the commands to compute a document-document similarity matrix over the above documents, in scikit-learn.\n","\n","Please use a processing [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) and a [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and compute the *cosine similarities* between all documents.  \n","\n","Note: With Tfidftransformer you will systematically compute word counts using CountVectorizer and then compute the Inverse Document Frequency (IDF) values and only then compute the TF-IDF scores.\n","\n","With Tfidfvectorizer on the contrary, you will do all three steps at once. Under the hood, it computes the word counts, IDF values, and TF-IDF scores all using the same dataset.\n","\n","General guideline re. how to use Tfidftransformer over Tfidfvectorizer :\n","\n","- If you need the term frequency (term count) vectors for different tasks, use Tfidftransformer.\n","- If you need to compute TF-IDF scores on documents within your “training” dataset, use Tfidfvectorizer\n","- If you need to compute TF-IDF scores on documents outside your “training” dataset, use either one, both will work.\n","\n","<font color=\"green\">**Question**: At the end, you will be asked to display the five most similar documents to the one you selected above, and compare the 1st and the 5th best results.</font>\n","\n","You can use inspiration from: \n"," * the above code\n"," * https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XkK2ceFCe-Y\n"," * https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\n"," * https://stackoverflow.com/questions/12118720/python-TF-IDF-cosine-to-find-document-similarity\n"," * https://markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1678792659832,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"9yC9hlqO3u7q"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer \n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1678792659832,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"8vi2v7Ml3u7q"},"outputs":[],"source":["tfidf = TfidfVectorizer(use_idf=True)\n","pipe = Pipeline(steps=[('pre', processor), ('tfidf', tfidf)]) # the 'processor' was defined above"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tPKeR2Rj3u7r"},"source":["<font color='green'>**Question**: Please write a function called `find_similar` which receives a `tfidf_matrix` with all similarity scores between documents, and the `index` of a document in the collection, and returns the `top_n` most similar documents to it using cosine similarity.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1678792659832,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"7sxkbsD13u7r"},"outputs":[],"source":["def find_similar(tfidf_matrix, index, top_n = 5):\n","# BEGIN_REMOVE\n","# END_REMOVE"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BUnaIpje3u7r"},"source":["<font color=\"green\">**Question**: Using the data from the Pandas form created above, please use \"fit\" and \"transform\" to generate the matrix of all document similarites called \"tfidf_matrix\". -- How long do these two operations take on your computer?  -- Please explain briefly in your own words what is the difference between \"fit\" and \"transform\".</font>"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1678792659833,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"ym6Labpm3u7r","outputId":"86187d87-310d-47fd-e492-1e26d224c77a"},"outputs":[],"source":["import time\n","# BEGIN_REMOVE\n","# END_REMOVE"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7fWaEBWW3u7r"},"source":["<font color=\"green\">**Question**: Using `find_similar` and the `tfidf_matrix` please display the five most similar documents to the one you selected above, with their scores, comment them, and compare the 1st and the 5th best results.</font>"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1678792659833,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"V-pVWIh43u7r","outputId":"afc0e3c3-55da-4906-df26-779671d0baae"},"outputs":[],"source":["# BEGIN_REMOVE\n","# END_REMOVE"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DC0mkVoe3u7s"},"source":["<font color='green'>**Question**: Could you also use the dot product instead of the cosine similarity in the `find_similar` function?  Please answer in the following box.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1678792659833,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"GAe_dkwF3u7s"},"outputs":[],"source":["# BEGIN_REMOVE\n","# END_REMOVE"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"O_8zazOh3u7s"},"source":["## Building a search engine using Gensim\n","\n","<font color='green'>**Question**: Using the [tutorial on Topics and Transformations from Gensim](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py), please implement a method that returns the documents most similar to a given query.\n","    \n","Use [Gensim's TF-IDF Model](https://radimrehurek.com/gensim/models/tfidfmodel.html) to build the model and the [MatrixSimilarity function](https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.MatrixSimilarity) to measure cosine similarity between documents.</font>\n","\n","<font color='green'>Please write a query of your own (5-10 words), retrieve the 5 most similar documents, and comment the result.</font>"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1678792659833,"user":{"displayName":"Daniel Perruchoud","userId":"15604693075568907580"},"user_tz":-60},"id":"Etqoo3qd3u7s","outputId":"b239e443-60e4-4105-f3d5-46622c40e873"},"outputs":[],"source":["# BEGIN_REMOVE\n","# END_REMOVE"]},{"cell_type":"markdown","metadata":{"id":"ms3Fdokq3u7t"},"source":["## End of Lab 4\n","Please make sure all cells have been executed, save this completed notebook, compress it to a *zip* file, and upload it to [Moodle](https://moodle.msengineering.ch/course/view.php?id=1869)."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
