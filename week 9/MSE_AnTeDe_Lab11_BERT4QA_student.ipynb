{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "attractive-statistics",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logocompact/300x300/1613732714/logo-mse.png \"MSE Logo\") \n",
    "\n",
    "# AnTeDe Lab11: Question Answering using BERT\n",
    "\n",
    "by Andrei Popescu-Belis (HES-SO)\n",
    "using the [ðŸ¤— Huggingface models](https://huggingface.co/models),\n",
    "an [article by Marius Borcan](https://programmerbackpack.com/bert-nlp-using-distilbert-to-build-a-question-answering-system/) and \n",
    "an [article by Ramsi Goutham](https://towardsdatascience.com/simple-and-fast-question-answering-system-using-huggingface-distilbert-single-batch-inference-bcf5a5749571)\n",
    "\n",
    "**Summary**\n",
    "The goal of this lab is to implement and test a simple question answering (QA) system over a set of articles.  The structure of the lab is as follows:\n",
    "1. Answer extraction from a text fragment -- in this part, you will use a pre-trained model named DistilBERT (a lighter version of BERT) which can extract the most likely answer to a given question from a text fragment (in English).\n",
    "2. Text retrieval given a question -- in this part, you will reuse code from Lab 4 (Search Engine) to design a paragraph retrieval system over the 300-article Lee corpus provided with `gensim`. \n",
    "3. Integration and testing -- in this part, you will put together the functions from the previous two parts, and test your system end-to-end by designing a test set of 10 questions.\n",
    "\n",
    "<font color='green'>Please answer the questions and solve the tasks in green within this notebook.  The expected answers are generally very short: 1-2 commands or 2-3 lines of explanations.  At the end, please submit the completed notebook under the corresponding homework on Moodle.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-karma",
   "metadata": {},
   "source": [
    "## 1. Answer extraction using DistilBERT\n",
    "\n",
    "As you know, the BERT pre-trained model can be fine-tuned for question answering, by training it to provide the start and end word of an input text fragment which is most likely the answer to an input question.  You will use the ðŸ¤— Huggingface Python module called `transformers`, and later use a DistilBERT model also provided by ðŸ¤— Huggingface.\n",
    "\n",
    "### a. Install `pytorch` and `transformers`\n",
    "\n",
    "Use the instructions provided by [PyTorch](https://pytorch.org/get-started/locally/#start-locally) and by [Huggingface](https://github.com/huggingface/transformers#installation).  The use of `conda` is recommended."
   ]
  },
  {
   "cell_type": "code",
   "id": "fitting-august",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:27.280921Z",
     "start_time": "2024-04-28T13:53:26.747597Z"
    }
   },
   "source": "import torch",
   "outputs": [],
   "execution_count": 1
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "macro-characteristic",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please generate a random 2x2x2 tensor with Pytorch.  Please display whether the workstation you use has a GPU or not.</font><br/>\n",
    "(Note: a GPU is not required for this lab.)"
   ]
  },
  {
   "cell_type": "code",
   "id": "complicated-spider",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:27.295945Z",
     "start_time": "2024-04-28T13:53:27.281720Z"
    }
   },
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device is set to: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is set to: mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:27.298495Z",
     "start_time": "2024-04-28T13:53:27.296504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random_tensor = torch.rand(2, 2, 2)\n",
    "print(random_tensor)"
   ],
   "id": "fa367f6455f1122a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7504, 0.7269],\n",
      "         [0.2922, 0.7640]],\n",
      "\n",
      "        [[0.1090, 0.9802],\n",
      "         [0.6623, 0.4873]]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "quantitative-pittsburgh",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:27.472617Z",
     "start_time": "2024-04-28T13:53:27.299446Z"
    }
   },
   "source": [
    "import transformers"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "normal-steering",
   "metadata": {},
   "source": [
    "ðŸ¤— Huggingface provides a very large repository of Transformer-based models at https://huggingface.co/models.\n",
    "\n",
    "<font color='green'>**Task**: Please use the search interface (in a browser) and find out *how many models containing the name 'distilbert' for Question Answering* are available.  If we exclude those submitted by individual users, how many models are there left?  Please paste below their name and version date, and the size of their 'pytorch_model.bin' file.</font>\n",
    "\n",
    "<font color='green'>By looking at their \"model cards\", which model has the highest performance on the SQuAD dev set?</font>  In what follows, we will use this model."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "There are 1,223 models available for Q&A with \"distilbert\" in their name.\n",
    "\n",
    "Excluding the models submitted by individual users, was not possible in the search interface...\n",
    "But the first model in this list is the one which is downloaded more and will be used in the lab.\n",
    "__________________________________________________________________________________________\n",
    " \n",
    " DistilBERT base cased distilled SQuAD (2020-02-07) \n",
    " - pytorch_model.bin size -> 261 MB\n",
    " - Downloads last month 227,317\n",
    "  - Model size 65.2M params\n",
    "- https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad\n",
    "- This model reaches a F1 score of 87.1 on the [SQuAD v1.1] dev set (for comparison, BERT bert-base-cased version reaches a F1 score of 88.7)\n",
    "\n",
    "__________ \n",
    "DistilBERT base uncased distilled SQuAD \n",
    "- pytorch_model.bin size  -> 265 MB\n",
    "- Downloads last month 39,626\n",
    "- Model size 66.4M params\n",
    "- https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad\n",
    "- This model reaches a F1 score of 86.9 on the [SQuAD v1.1] dev set (for comparison, Bert bert-base-uncased version reaches a F1 score of 88.5\n"
   ],
   "id": "611c57c05f13aa18"
  },
  {
   "cell_type": "markdown",
   "id": "explicit-light",
   "metadata": {},
   "source": [
    "### b. Tokenization of the input\n",
    "\n",
    "We will use here a tokenizer called `DistilBertTokenizer` to tokenize the question and the text fragment and transform the numbers into numerical indices.  The documentation for this tokenizer is included in the general documentation of DistilBERT models at: https://huggingface.co/transformers/model_doc/distilbert.html "
   ]
  },
  {
   "cell_type": "code",
   "id": "adequate-accountability",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:27.538178Z",
     "start_time": "2024-04-28T13:53:27.473223Z"
    }
   },
   "source": "from transformers import DistilBertTokenizer",
   "outputs": [],
   "execution_count": 5
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "hungarian-beginning",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please create an instance of such a tokenizer \n",
    "using the pre-trained model named 'distilbert-base-cased'.  The command\n",
    "will download the necessary model the first time you use it.</font>"
   ]
  },
  {
   "cell_type": "code",
   "id": "assured-andorra",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:27.752056Z",
     "start_time": "2024-04-28T13:53:27.538752Z"
    }
   },
   "source": "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')",
   "outputs": [],
   "execution_count": 6
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "complete-pontiac",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: What does this instance return if you **call** it with a sentence (a *string*) as an argument?  Please write the instruction below for the sentence 'There are three museums in Winterthur.'.</font>"
   ]
  },
  {
   "cell_type": "code",
   "id": "biological-relief",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:27.755813Z",
     "start_time": "2024-04-28T13:53:27.752740Z"
    }
   },
   "source": [
    "sentence = \"There are three museums in Winterthur.\"\n",
    "tokenizer_output = tokenizer(sentence)\n",
    "tokenizer_output"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1247, 1132, 1210, 11765, 1107, 4591, 1582, 2149, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "offshore-navigation",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please explain in your own words the meaning of the two components of the output above.  For that, please use the [documentation of the class DistilBertTokenizer](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer), and be sure you read the documentation of its *superclasses* as well.  Under what superclass do you find the links to the [glossary entries](https://huggingface.co/transformers/glossary.html) that best explain the two components, and what are these entries?</font>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The first output \"input_ids\" of the tokenizer is the list of tokens obtained by splitting the input sentence into tokens available in the tokenizer vocabulary. These tokens are then represented in ids which is understandable by the model.\n",
    "\n",
    "The second output \"attention_mask\": \n",
    "If two sequences do not have the same length, they need to be padded to the same. These padding words are not relevant for the model so in the attention_mask they are represented as a 0 and relevant words as a 1.\n",
    "\n",
    "The superclass where I found the link to the glossary was the PreTrainedTokenizer.\n",
    "Gloassary: \n",
    "\n",
    "input_ids -> https://huggingface.co/transformers/v3.1.0/glossary.html#input-ids\n",
    "attention_mask -> https://huggingface.co/transformers/v3.1.0/glossary.html#attention-mask\n"
   ],
   "id": "24c8529cc608390d"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "expected-empty",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: If you haven't explained above, please explain here the cause of the difference between the number of words of your sentence, and the number of tokens in the observed output.  Please display the tokens of the output. You can use the documentation of the superclass found above or the examples in the [glossary](https://huggingface.co/transformers/glossary.html).</font>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:27.757569Z",
     "start_time": "2024-04-28T13:53:27.756302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert token IDs to tokens and store them in a variable\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer_output[\"input_ids\"])\n",
    "# Print tokens\n",
    "print(tokens)"
   ],
   "id": "641ea4793dc4e5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'There', 'are', 'three', 'museums', 'in', 'Winter', '##th', '##ur', '.', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The difference in the number of words and tokens is due to the tokenization process. The tokenizer splits the input sentence into tokens, and these tokens are then represented in ids which are understandable by the model. The tokenizer also adds special tokens such as [CLS] and [SEP] to the input sentence. These special tokens are not part of the input sentence but are added by the tokenizer to help the model understand the input sentence better.",
   "id": "1cfa64358dfc583"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "general-bulgarian",
   "metadata": {},
   "source": [
    "<font color='green'>**Question**: How can you convert back the first part of the output to the original string?\n",
    "Please write and execute the command(s) below.  You can use the documentation of the superclass found above or the examples in the [glossary](https://huggingface.co/transformers/glossary.html).</font>"
   ]
  },
  {
   "cell_type": "code",
   "id": "dietary-prototype",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:27.759454Z",
     "start_time": "2024-04-28T13:53:27.758055Z"
    }
   },
   "source": [
    "original_tokens = tokenizer.convert_ids_to_tokens(tokenizer_output['input_ids'])\n",
    "original_sentence = tokenizer.convert_tokens_to_string(original_tokens)\n",
    "print(f'The original string was:\\n{original_sentence}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original string was:\n",
      "[CLS] There are three museums in Winterthur . [SEP]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "resident-multiple",
   "metadata": {},
   "source": [
    "### c. Generation of input in the desired form\n",
    "\n",
    "We need to generate input in the form expected by the `DistilBertForQuestionAnswering` class.  This means providing the question, the text from which the answer must be extracted, with the proper [CLS] and [SEP] tokens, and the attention masks.  Moreover, using DistilBERT requires that the lists of indices returned by the tokenizer are Pytorch tensors (see tokenizer's option `return_tensors`).\n",
    "\n",
    "<font color='green'>**Question**: What is the correct way to call the tokenizer in order to obtain these results?  You can use the example provided at the end of the [DistilBertForQuestionAnswering](https://huggingface.co/transformers/model_doc/distilbert.html?distilbertforquestionanswering#distilbertforquestionanswering) documentation.  <br/>Please define a *question* and a *text* string of your own, and store the result of the tokenizer in a variable called *input*.  <br/>   Please verify (by converting back to the result) that the input has the correct tokens.</font>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:28.080517Z",
     "start_time": "2024-04-28T13:53:27.761025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "question, text = \"What are people struggling with?\", \"Many people do not know how to vote at 9th June about the national initiatives\"\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "input_sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "print(f'The original input was: \\n -> {question} {text} \\nand the converted back input is: \\n -> {input_sentence}')"
   ],
   "id": "fd8ee7a5dfcf4d1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original input was: \n",
      " -> What are people struggling with? Many people do not know how to vote at 9th June about the national initiatives \n",
      "and the converted back input is: \n",
      " -> [CLS] What are people struggling with ? [SEP] Many people do not know how to vote at 9th June about the national initiatives [SEP]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "structured-parts",
   "metadata": {},
   "source": [
    "### d. Execution of the model over the input question and text\n",
    "\n",
    "In this section, you will create an instance of the BERT neural network adapted to question answering.  The class is named `DistilBertForQuestionAnswering`.  \n",
    "\n",
    "**Important note:** The model itself (the weights) is the one that you found at the end of (1a) above which is suited for question answering!"
   ]
  },
  {
   "cell_type": "code",
   "id": "demographic-attention",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:28.350428Z",
     "start_time": "2024-04-28T13:53:28.081139Z"
    }
   },
   "source": [
    "from transformers import DistilBertForQuestionAnswering"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "located-dayton",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please create an instance of the model here.</font>  The data will be downloaded the first time you create it. "
   ]
  },
  {
   "cell_type": "code",
   "id": "median-quebec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:28.893768Z",
     "start_time": "2024-04-28T13:53:28.351094Z"
    }
   },
   "source": "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")",
   "outputs": [],
   "execution_count": 12
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ultimate-firmware",
   "metadata": {},
   "source": [
    "The results of applying the model to your question and text (i.e. extracting the answer) are obtained by calling the model with the correct inputs.  \n",
    "\n",
    "<font color='green'>**Task**: Please use the inputs you obtained above and read the [documentation of the DistilBertForQuestionAnswering class](https://huggingface.co/transformers/model_doc/distilbert.html?distilbertforquestionanswering#distilbertforquestionanswering) (under *forward*) to apply the model to your data.  Store the results in a variable called *outputs*.</font>"
   ]
  },
  {
   "cell_type": "code",
   "id": "norman-staff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:29.103162Z",
     "start_time": "2024-04-28T13:53:28.894516Z"
    }
   },
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "coated-guitar",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please answer the following three questions: \n",
    "- Where are the probability values for the position of the **start** of the answer in *outputs*?\n",
    "- Are these actual probabilities or other type of coefficients?  \n",
    "- How many values are there, and is this coherent with your observations in (1b)?</font> "
   ]
  },
  {
   "cell_type": "code",
   "id": "coral-mozambique",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:29.107192Z",
     "start_time": "2024-04-28T13:53:29.103957Z"
    }
   },
   "source": [
    "print(f'Probability values for the position of the start of the answer: {outputs.start_logits}')\n",
    "\n",
    "# They are not actual probabilities but logits.\n",
    "\n",
    "print(\n",
    "    f'The number of values is {len(outputs.start_logits[0])} which is coherent with the number of tokens in the input.')\n",
    "print(\n",
    "    f'There are {len(outputs.start_logits[0])} values which is the same as the input token-size {len(input_ids[0])}')  # There are "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability values for the position of the start of the answer: tensor([[ -7.7410,  -8.3506, -10.8415, -10.3164,  -9.4377, -10.1578,  -8.6988,\n",
      "          -8.4618,  -1.5717,  -3.6089,  -4.3863,  -5.0113,  -6.3410,  -5.3681,\n",
      "          -6.4108,  -3.0860,  -6.4063,  -3.7960,  -7.2373,  -3.7928,   0.4913,\n",
      "           0.2110,  -2.1361,  -8.4617]])\n",
      "The number of values is 24 which is coherent with the number of tokens in the input.\n",
      "There are 24 values which is the same as the input token-size 24\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "horizontal-ridge",
   "metadata": {},
   "source": [
    "### e. Determination of the start and the end of the answer in the text\n",
    "\n",
    "<font color='green'>**Question**: Please use the *outputs* of the model to determine the most likely start and end of the answer span in your text, and then obtain the actual answer.  How satisfied are you with the answer?</font>  You may use help from the [ðŸ¤— Huggingface entry on question answering](https://huggingface.co/transformers/task_summary.html#extractive-question-answering)."
   ]
  },
  {
   "cell_type": "code",
   "id": "expected-closer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:29.110024Z",
     "start_time": "2024-04-28T13:53:29.107838Z"
    }
   },
   "source": [
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_token_ids = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n",
    "answer_sentence = tokenizer.decode(predict_answer_token_ids)\n",
    "print(f'Answer: {answer_sentence}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: the national initiatives\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The question was answered corrected even with the correct and important adjective \"national\".",
   "id": "794d474f00dec048"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "subsequent-hours",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please write a function called *answer_extraction* that gathers the previous operations: it takes two strings as arguments, creates instances of the tokenizer and the model, extracts the answer, and returns it as a string (possibly empty).  Do not create a new *tokenizer* and *model*, but assume that the ones you created above are global variables accessible from this function.</font> "
   ]
  },
  {
   "cell_type": "code",
   "id": "atomic-overview",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:29.112347Z",
     "start_time": "2024-04-28T13:53:29.110624Z"
    }
   },
   "source": [
    "def answer_extraction(question, text):\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "    predict_answer_token_ids = inputs.input_ids[0, answer_start_index: answer_end_index + 1]\n",
    "\n",
    "    return tokenizer.decode(predict_answer_token_ids)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "approximate-samoa",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please test the function on the following questions and short text.</font> "
   ]
  },
  {
   "cell_type": "code",
   "id": "martial-precipitation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:29.185450Z",
     "start_time": "2024-04-28T13:53:29.113084Z"
    }
   },
   "source": [
    "# Excerpt from Simple English Wikipedia:\n",
    "text = \"\"\"Switzerland is a small country in Western Europe. \n",
    "Switzerland is a confederation of even smaller states, which are the 26 cantons.\n",
    "Switzerland is known for its neutrality.  Switzerland has been neutral since 1815. \n",
    "There are four official languages in Switzerland: German, French, Italian, and Romansh. \n",
    "\"\"\"\n",
    "question1 = \"How many cantons are there in Switzerland?\"\n",
    "question2 = \"What is Switzerland famous for?\"\n",
    "question3 = \"What are the official languages of Switzerland?\"\n",
    "\n",
    "print(question1, '->', answer_extraction(question1, text))\n",
    "print(question2, '->', answer_extraction(question2, text))\n",
    "print(question3, '->', answer_extraction(question3, text))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many cantons are there in Switzerland? -> 26\n",
      "What is Switzerland famous for? -> neutrality\n",
      "What are the official languages of Switzerland? -> German, French, Italian, and Romansh\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "vital-authorization",
   "metadata": {},
   "source": [
    "## 2. Fragment retrieval using `Gensim` (from Lab 4)\n",
    "\n",
    "In this part, you will simply reuse code from Lab 4 to build a simple text retrieval system over the *Lee Corpus* provided with Gensim (300 news articles from the Australian Broadcasting Corporation).  \n",
    "* The [Gensim tutorial on topics and transformations](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py) provides the main idea.  \n",
    "* The goal is to retrieve, given a question, a short text fragment that is most likely to contain the answer.  As articles are not divided into paragraphs, you will refactor the collection of articles into a collection of fragments of at most *N* sentences each (without mixing articles). \n",
    "* The question will be used as a *query*, with the pre-processing options of your choice."
   ]
  },
  {
   "cell_type": "code",
   "id": "fourth-wrestling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:29.758237Z",
     "start_time": "2024-04-28T13:53:29.186250Z"
    }
   },
   "source": "import gensim, nltk, os",
   "outputs": [],
   "execution_count": 18
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "furnished-introduction",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Load the articles of the Lee Background Corpus provided with Gensim into a list of strings (each article in a string) called *raw_articles*.</font>"
   ]
  },
  {
   "cell_type": "code",
   "id": "skilled-batch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:29.761310Z",
     "start_time": "2024-04-28T13:53:29.758755Z"
    }
   },
   "source": [
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "raw_articles = open(lee_train_file).read().splitlines()\n",
    "\n",
    "print(f'Number of articles: {len(raw_articles)}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 300\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:29.764343Z",
     "start_time": "2024-04-28T13:53:29.761871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save articles to file\n",
    "with open('articles.txt', 'w') as f:\n",
    "    for article in raw_articles:\n",
    "        f.write(article + '\\n')"
   ],
   "id": "12fefff431254edc",
   "outputs": [],
   "execution_count": 20
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "strong-collins",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please transform the articles into a collection of text fragments called *corpus1* (a list of lists of strings), by cutting each article into fragments of *N* consecutive sentences (e.g. *N* = 4), except possibly for the last fragment, and tokenizing each sentence.  At the end, display the number of fragments of your collection.</font>\n",
    "* Do not mix sentences from different articles in each fragment.\n",
    "* The reason for this operation is that full articles are too long to give to DistilBERT as texts. (Try it!)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:29.848044Z",
     "start_time": "2024-04-28T13:53:29.764726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answer_extraction(raw_articles[0], 'What happened in the Southern Highlands of New South Wales?')\n",
    "# Always output [CLS] token, because the input is too long for the model."
   ],
   "id": "14d6b51e53f65373",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e8e1051",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Do not forget to pre-process the articles in preparation for search -- tokenization, stopword removal, and other operations if you want to explore them.</font>  \n",
    "* A  text fragment is thus a list of strings (tokens). \n",
    "* Please inspect your corpus to make sure it is correctly built."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ead35154",
   "metadata": {},
   "source": [
    "Hint: the raw text will start with:\n",
    " \n",
    "*Hundreds of people have been forced to vacate their homes in the Southern Highlands of New South Wales as strong winds today pushed a huge bushfire towards the town of Hill Top. A new blaze near Goulburn, south-west of Sydney, has forced the closure of the Hume Highway. At about 4:00pm AEDT, a marked deterioration in the weather as a storm cell moved east across the Blue Mountains forced authorities to make a decision to evacuate people from homes in outlying streets at Hill Top in the New South Wales southern highlands. An estimated 500 residents have left their homes for nearby Mittagong. The New South Wales Rural Fire Service says the weather conditions which caused the fire to burn in a finger formation have now eased and about 60 fire units in and around Hill Top are optimistic of defending all properties. As more than 100 blazes burn on New Year's Eve in New South Wales, fire crews have been called to new fire at Gunning, south of Goulburn. While few details are available at this stage, fire authorities says it has closed the Hume Highway in both directions. Meanwhile, a new fire in Sydney's west is no longer threatening properties in the Cranebrook area. ....*\n",
    "\n",
    "The first 8 sentences are to be decomposed into:\n",
    "\n",
    "*[['hundreds', 'people', 'forced', 'vacate', 'homes', 'southern', 'highlands', 'new', 'south', 'wales', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'town', 'hill', 'top', '.', 'new', 'blaze', 'near', 'goulburn', ',', 'south-west', 'sydney', ',', 'forced', 'closure', 'hume', 'highway', '.', '4:00pm', 'aedt', ',', 'marked', 'deterioration', 'weather', 'storm', 'cell', 'moved', 'east', 'across', 'blue', 'mountains', 'forced', 'authorities', 'make', 'decision', 'evacuate', 'people', 'homes', 'outlying', 'streets', 'hill', 'top', 'new', 'south', 'wales', 'southern', 'highlands', '.', 'estimated', '500', 'residents', 'left', 'homes', 'nearby', 'mittagong', '.'], ['new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'weather', 'conditions', 'caused', 'fire', 'burn', 'finger', 'formation', 'eased', '60', 'fire', 'units', 'around', 'hill', 'top', 'optimistic', 'defending', 'properties', '.', '100', 'blazes', 'burn', 'new', 'year', 'eve', 'new', 'south', 'wales', ',', 'fire', 'crews', 'called', 'new', 'fire', 'gunning', ',', 'south', 'goulburn', '.', 'details', 'available', 'stage', ',', 'fire', 'authorities', 'says', 'closed', 'hume', 'highway', 'directions', '.', 'meanwhile', ',', 'new', 'fire', 'sydney', 'west', 'longer', 'threatening', 'properties', 'cranebrook', 'area', '.']*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e1d182e",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: If possible, store the original version of each fragment into a string in *corpus2* (i.e. non-tokenized, non-lowercased, etc.), because it will be better to pass it later to DistilBERT.  Otherwise, you can also reconstruct the full fragment from the tokens in corpus1.</font>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:31.713541Z",
     "start_time": "2024-04-28T13:53:29.848823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "import copy\n",
    "\n",
    "corpus1 = []\n",
    "corpus2 = []\n",
    "N = 4\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    nltk_tokens = nltk.word_tokenize(text)\n",
    "    nltk_tokens = [word.lower() for word in nltk_tokens if word.isalnum()]\n",
    "    return [word for word in nltk_tokens if word not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "for article in raw_articles:\n",
    "    sentences = nltk.sent_tokenize(article)\n",
    "    fragment_list = []\n",
    "    for i in range(0, len(sentences), N):\n",
    "        fragment = \" \".join(sentences[i:i + N])\n",
    "        corpus2.append(fragment)\n",
    "        corpus1.append(preprocess_text(fragment))"
   ],
   "id": "a563131da201f5bd",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:31.715819Z",
     "start_time": "2024-04-28T13:53:31.714219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Number of fragments: {len(corpus1)} \\n--------------------------')\n",
    "print(f'Raw first fragment:\\n {corpus2[0]} \\n')\n",
    "print(f'Processed first fragment:\\n {corpus1[0]}')"
   ],
   "id": "e8efe9ab2dcf42c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fragments: 790 \n",
      "--------------------------\n",
      "Raw first fragment:\n",
      " Hundreds of people have been forced to vacate their homes in the Southern Highlands of New South Wales as strong winds today pushed a huge bushfire towards the town of Hill Top. A new blaze near Goulburn, south-west of Sydney, has forced the closure of the Hume Highway. At about 4:00pm AEDT, a marked deterioration in the weather as a storm cell moved east across the Blue Mountains forced authorities to make a decision to evacuate people from homes in outlying streets at Hill Top in the New South Wales southern highlands. An estimated 500 residents have left their homes for nearby Mittagong. \n",
      "\n",
      "Processed first fragment:\n",
      " ['hundreds', 'people', 'forced', 'vacate', 'homes', 'southern', 'highlands', 'new', 'south', 'wales', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'town', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'sydney', 'forced', 'closure', 'hume', 'highway', 'aedt', 'marked', 'deterioration', 'weather', 'storm', 'cell', 'moved', 'east', 'across', 'blue', 'mountains', 'forced', 'authorities', 'make', 'decision', 'evacuate', 'people', 'homes', 'outlying', 'streets', 'hill', 'top', 'new', 'south', 'wales', 'southern', 'highlands', 'estimated', '500', 'residents', 'left', 'homes', 'nearby', 'mittagong']\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "everyday-canadian",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please create a search index (called *search_index*) using a *tfidf* model and transform all text fragments from *corpus1* into document vectors.</font>"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:31.787224Z",
     "start_time": "2024-04-28T13:53:31.716543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Assume that corpus1 is a list of tokenized documents\n",
    "dictionary = corpora.Dictionary(corpus1)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in corpus1]\n",
    "\n",
    "# Create a tf-idf model and transform the corpus to tf-idf\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "\n",
    "# Create a similarity index\n",
    "search_index = similarities.MatrixSimilarity(tfidf_corpus)"
   ],
   "id": "7814518fc17d4d8",
   "outputs": [],
   "execution_count": 24
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "certified-pitch",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please write a function called *fragment_retrieval* which returns the most relevant text fragment (string) from the corpus given a question, which is used as the query.</font>  \n",
    "* The function processes the query in the same way as the documents (using the *tfidf model*) to obtain a *vectorized_query*.\n",
    "* This is passed to the *search_index* to rank all documents by relevance.\n",
    "* All the resources created above are supposed available as global variables (the dictionary, the tfidf model, the search_index, the corpus)."
   ]
  },
  {
   "cell_type": "code",
   "id": "disturbed-economics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:31.789539Z",
     "start_time": "2024-04-28T13:53:31.787779Z"
    }
   },
   "source": [
    "def fragment_retrieval(query):\n",
    "    tokenized_query = preprocess_text(query)\n",
    "    tfidf_query = tfidf[dictionary.doc2bow(tokenized_query)]\n",
    "    similarities = search_index[tfidf_query]\n",
    "    return corpus2[similarities.argmax()]"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "agricultural-rapid",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please apply the above function to the three queries provided below.</font>  \n",
    "\n",
    "Note: again, the corpus, search_index, tfidf and dictionary are available as global variables."
   ]
  },
  {
   "cell_type": "code",
   "id": "automotive-recognition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:31.807995Z",
     "start_time": "2024-04-28T13:53:31.790043Z"
    }
   },
   "source": [
    "queries = [\"Who is the mayor of New York?\",\n",
    "           \"Who is Nicole Kidman?\",\n",
    "           \"How many Australians died in the 1999 Interlaken canyoning accident?\"]\n",
    "for q in queries:\n",
    "    print(q, '->', fragment_retrieval(q))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the mayor of New York? -> \"I felt that my job as the mayor was to turn around the city, because I believed - rightly or wrongly - that we had one last chance to do that.\" Mr Giuliani, a Republican, has served two terms as New York City's Mayor since 1993. Term limits prevent him from seeking a third term in office, and he will be succeeded by billionaire media mogul Michael Bloomberg.\n",
      "Who is Nicole Kidman? -> In the United States, Australian actress Nicole Kidman has been nominated for two Golden Globe best actor awards for her roles in the Australian-made musical \"Moulin Rouge\", and in her new thriller \"The Others\". \"Moulin Rouge\" also is one of two pictures leading the Golden Globe nominations, with six possible awards. It is vying for best musical or comedy picture of 2002, best actress in a comedy or musical, best actor in the same category for Ewen McGregor, best director for Baz Luhrmann, best original score and best original song. The other film to pick up six nominations is the Ron Howard directed \"A Beautiful Mind\" starring Oscar winner Russell Crowe.\n",
      "How many Australians died in the 1999 Interlaken canyoning accident? -> Eight people are to appear in a Swiss court tomorrow charged with the manslaughter of 18 tourists and three guides, after the 1999 Interlaken canyoning tragedy. The first three defendants are managers of the now defunctoperator, Adventure World. Twenty-one people including 14 Australians were killed when a thunderstorm struck when they were canyoning down the Saxeten River Gorge near Interlaken. A massive wall of water hit the group and swept them to their deaths.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "closed-clause",
   "metadata": {},
   "source": [
    "## 3. Integration, testing and discussion\n",
    "\n",
    "<font color='green'>**Task**: Using the two functions 'fragment_retrieval' and 'answer_extraction' from parts 1 and 2, and assuming all models and data are available as global variables, please create a unique function which returns the answer (string) to a question (string).</font>"
   ]
  },
  {
   "cell_type": "code",
   "id": "metallic-dollar",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:31.846498Z",
     "start_time": "2024-04-28T13:53:31.815294Z"
    }
   },
   "source": [
    "def question_answering(question):\n",
    "    fragment = fragment_retrieval(question)\n",
    "    return answer_extraction(question, fragment)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "purple-impact",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please add between 5 and 10 more questions to the following list.  You can add answerable and non-answerable questions (with respect to the corpus).</font>"
   ]
  },
  {
   "cell_type": "code",
   "id": "suburban-commercial",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T13:53:33.595047Z",
     "start_time": "2024-04-28T13:53:31.875665Z"
    }
   },
   "source": [
    "questions = [\"Who is the mayor of New York?\",\n",
    "             \"Who is Nicole Kidman?\",\n",
    "             \"How many Australians died in the 1999 Interlaken canyoning accident?\",\n",
    "             \"What caused the 1999 Interlaken canyoning accident?\",\n",
    "             \"Which city is the capital of Australia?\",\n",
    "             \"Who is the prime-minister of Israel?\",\n",
    "             \"What are the main Australian airlines?\",\n",
    "             \"What is Kieren Perkins' sport?\",\n",
    "             \"What is the population of Australia?\",\n",
    "             \"What is the capital of Haiti?\",\n",
    "             \"Who is Anthony Zinni?\",\n",
    "             \"Where is the opera house?\",\n",
    "             ]\n",
    "for q in questions:\n",
    "    print(q, '->', question_answering(q))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the mayor of New York? -> Mr Giuliani\n",
      "Who is Nicole Kidman? -> Australian actress\n",
      "How many Australians died in the 1999 Interlaken canyoning accident? -> 14\n",
      "What caused the 1999 Interlaken canyoning accident? -> thunderstorm\n",
      "Which city is the capital of Australia? -> Port - au - Prince\n",
      "Who is the prime-minister of Israel? -> Anthony Zinni\n",
      "What are the main Australian airlines? -> Perth and Darwin\n",
      "What is Kieren Perkins' sport? -> swam\n",
      "What is the population of Australia? -> aging\n",
      "What is the capital of Haiti? -> Port - au - Prince\n",
      "Who is Anthony Zinni? -> United States peace envoy\n",
      "Where is the opera house? -> Palestinian territory\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fabulous-active",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please discuss the correctness of the answers, give possible reasons for incorrect ones, and make suggestions for improvements.</font>\n",
    "\n",
    "Write your discussion here or in a cell below.\n",
    "\n",
    "When you have finished please clean and re-run one last time the notebook, from start to end, then submit it on Moodle."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Discussion:\n",
    "Most questions are answered correct, but the question on the capitol of australia is not. This is probably because in one article the word capitol is used twice, but in correlation to Haiti and its capitol and not about the capitol of Australia. Thats why it answers with Port - au - Prince.\n",
    "\n",
    "The question on the prime minister of Israel is answered wrongly too, because of a similar reason. The name Anthony Zinni is often used in the articles in combination with other words such as prime-minister and Israel.\n",
    "When the model was asked directly \"Who is Anthony Zinni?\" it answered it correctly.\n",
    "\n",
    "The question on the main Australian airlines is not answered correctly, I would guess that in the context of airlines and Australia Perth and Darvin are probably the busiest airports in Australia, and mentioned quiet often in the articles in combination with airline. So the model might have picked up on that.\n",
    "\n",
    "The question on the population of Australia is not answered correctly, its interesting to see that the answer  would answer another question on the population of Australia: \"How is the demographic development in Australia?\". -> aging\n",
    "\n",
    "The answer to \"Where is the opera house?\" is totally wrong. But the word \"opera house\" never occurred in the texts, so the model could not have answered this question correctly. \n",
    "\n"
   ],
   "id": "ea5f17743936e53f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msenlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
