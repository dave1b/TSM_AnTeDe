{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "attractive-statistics",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logocompact/300x300/1613732714/logo-mse.png \"MSE Logo\") \n",
    "\n",
    "# AnTeDe Lab11: Question Answering using BERT\n",
    "\n",
    "by Andrei Popescu-Belis (HES-SO)\n",
    "using the [ðŸ¤— Huggingface models](https://huggingface.co/models),\n",
    "an [article by Marius Borcan](https://programmerbackpack.com/bert-nlp-using-distilbert-to-build-a-question-answering-system/) and \n",
    "an [article by Ramsi Goutham](https://towardsdatascience.com/simple-and-fast-question-answering-system-using-huggingface-distilbert-single-batch-inference-bcf5a5749571)\n",
    "\n",
    "**Summary**\n",
    "The goal of this lab is to implement and test a simple question answering (QA) system over a set of articles.  The structure of the lab is as follows:\n",
    "1. Answer extraction from a text fragment -- in this part, you will use a pre-trained model named DistilBERT (a lighter version of BERT) which can extract the most likely answer to a given question from a text fragment (in English).\n",
    "2. Text retrieval given a question -- in this part, you will reuse code from Lab 4 (Search Engine) to design a paragraph retrieval system over the 300-article Lee corpus provided with `gensim`. \n",
    "3. Integration and testing -- in this part, you will put together the functions from the previous two parts, and test your system end-to-end by designing a test set of 10 questions.\n",
    "\n",
    "<font color='green'>Please answer the questions and solve the tasks in green within this notebook.  The expected answers are generally very short: 1-2 commands or 2-3 lines of explanations.  At the end, please submit the completed notebook under the corresponding homework on Moodle.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-karma",
   "metadata": {},
   "source": [
    "## 1. Answer extraction using DistilBERT\n",
    "\n",
    "As you know, the BERT pre-trained model can be fine-tuned for question answering, by training it to provide the start and end word of an input text fragment which is most likely the answer to an input question.  You will use the ðŸ¤— Huggingface Python module called `transformers`, and later use a DistilBERT model also provided by ðŸ¤— Huggingface.\n",
    "\n",
    "### a. Install `pytorch` and `transformers`\n",
    "\n",
    "Use the instructions provided by [PyTorch](https://pytorch.org/get-started/locally/#start-locally) and by [Huggingface](https://github.com/huggingface/transformers#installation).  The use of `conda` is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "fitting-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "macro-characteristic",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please generate a random 2x2x2 tensor with Pytorch.  Please display whether the workstation you use has a GPU or not.</font><br/>\n",
    "(Note: a GPU is not required for this lab.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "id": "complicated-spider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4421, 0.2892],\n",
      "         [0.7718, 0.8245]],\n",
      "\n",
      "        [[0.2870, 0.4521],\n",
      "         [0.1176, 0.9186]]])\n"
     ]
    }
   ],
   "source": [
    "# Generate and print a tensor with random values between 0 and 1 of shape (2,2,2)\n",
    "tensor = torch.rand(2,2,2)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "6e11a978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Check whether a GPU is available or not\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "quantitative-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "normal-steering",
   "metadata": {},
   "source": [
    "ðŸ¤— Huggingface provides a very large repository of Transformer-based models at https://huggingface.co/models.\n",
    "\n",
    "<font color='green'>**Task**: Please use the search interface (in a browser) and find out *how many models containing the name 'distilbert' for Question Answering* are available.  If we exclude those submitted by individual users, how many models are there left?  Please paste below their name and version date, and the size of their 'pytorch_model.bin' file.</font>\n",
    "\n",
    "<font color='green'>By looking at their \"model cards\", which model has the highest performance on the SQuAD dev set?</font>  In what follows, we will use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "requested-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As of writing this comment, there are a total of 1222 available question answering models with \"distilbert\" in their name. \n",
    "\n",
    "# Excluding models submitted by individual users there seem to be 2 models left.\n",
    "\n",
    "# - DistilBERT base cased distilled SQuAD \n",
    "#   Last updated on the 12th of April 2023\n",
    "#   \"pytorch_model.bin\" file size = 261 mb\n",
    "#   Answer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160\n",
    "\n",
    "# - DistilBERT base uncased distilled SQuAD\n",
    "#   Last updated on the 6th of April 2023\n",
    "#   \"pytorch_model.bin\" file size = 265 mb\n",
    "#   Answer: 'SQuAD dataset', score: 0.4704, start: 147, end: 160 \n",
    "\n",
    "# The first \"cased\" model has a slightly higher score of 0.5152\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-light",
   "metadata": {},
   "source": [
    "### b. Tokenization of the input\n",
    "\n",
    "We will use here a tokenizer called `DistilBertTokenizer` to tokenize the question and the text fragment and transform the numbers into numerical indices.  The documentation for this tokenizer is included in the general documentation of DistilBERT models at: https://huggingface.co/transformers/model_doc/distilbert.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "adequate-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "hungarian-beginning",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please create an instance of such a tokenizer \n",
    "using the pre-trained model named 'distilbert-base-cased'.  The command\n",
    "will download the necessary model the first time you use it.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "id": "assured-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "complete-pontiac",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: What does this instance return if you **call** it with a sentence (a *string*) as an argument?  Please write the instruction below for the sentence 'There are three museums in Winterthur.'.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "biological-relief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformer returned something of type: <class 'transformers.tokenization_utils_base.BatchEncoding'> \n",
      "\n",
      "{'input_ids': [101, 1247, 1132, 1210, 11765, 1107, 4591, 1582, 2149, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Define a sentence and call the transformer instance with the sentence as an argument\n",
    "sentence = \"There are three museums in Winterthur\"\n",
    "transf_return = tokenizer(sentence)\n",
    "\n",
    "# Check what the transformer instance returned\n",
    "print(f\"The transformer returned something of type: {type(transf_return)} \\n\")\n",
    "print(transf_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "id": "e237e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned data structure is a dictionar with two keys."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "offshore-navigation",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please explain in your own words the meaning of the two components of the output above.  For that, please use the [documentation of the class DistilBertTokenizer](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer), and be sure you read the documentation of its *superclasses* as well.  Under what superclass do you find the links to the [glossary entries](https://huggingface.co/transformers/glossary.html) that best explain the two components, and what are these entries?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "diagnostic-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behind the first key \"inut_ids\" are the token IDs for the input text.\n",
    "# There are more IDs than individual words, because the tokenizer splits into subwords.\n",
    "# Behind the second key \"attention_,ask\" is an attention mask. It indicates which tokens should be attended to by the model.\n",
    "# This is needed if multiple sentences are to be stored in the same tensor and some need to be padded to have identical lengths.\n",
    "\n",
    "# I managed to find a hint about input_ids and the attention mask on the tokenizer page.\n",
    "# https://huggingface.co/docs/transformers/v4.40.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer\n",
    "\n",
    "# The actual links to the glossary for these two terms I managed to find on the DistilBERT page under DistilBERTModel:\n",
    "# https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertModel.forward\n",
    "\n",
    "# I however found the page very difficult to navigate and find this information in the first place.\n",
    "# If there is a very simple solution to this, then I would be curious to know.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "expected-empty",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: If you haven't explained above, please explain here the cause of the difference between the number of words of your sentence, and the number of tokens in the observed output.  Please display the tokens of the output. You can use the documentation of the superclass found above or the examples in the [glossary](https://huggingface.co/transformers/glossary.html).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "ef1e1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As stated above, the difference stems from the tokenizer splitting words into subwords.\n",
    "# Below it can be seen, that it split the unknown word \"Winterthur\" into 3 tokens and also added the [CLS] and [SEP] tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "first-boutique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'There', 'are', 'three', 'museums', 'in', 'Winter', '##th', '##ur', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Store the list of input IDs in a variable\n",
    "input_ids = transf_return[\"input_ids\"]\n",
    "\n",
    "# Convert token IDs to tokens and store them in a variable\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "# Print tokens\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "general-bulgarian",
   "metadata": {},
   "source": [
    "<font color='green'>**Question**: How can you convert back the first part of the output to the original string?\n",
    "Please write and execute the command(s) below.  You can use the documentation of the superclass found above or the examples in the [glossary](https://huggingface.co/transformers/glossary.html).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "id": "dietary-prototype",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] There are three museums in Winterthur [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Convert the input_ids back to the original sentence using the \"decode\" method\n",
    "orig_sentence = tokenizer.decode(input_ids)\n",
    "print(orig_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "620fafb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] There are three museums in Winterthur [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Alternative using the \"convert_tokens_to_string\" method\n",
    "orig_sentence = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(orig_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "resident-multiple",
   "metadata": {},
   "source": [
    "### c. Generation of input in the desired form\n",
    "\n",
    "We need to generate input in the form expected by the `DistilBertForQuestionAnswering` class.  This means providing the question, the text from which the answer must be extracted, with the proper [CLS] and [SEP] tokens, and the attention masks.  Moreover, using DistilBERT requires that the lists of indices returned by the tokenizer are Pytorch tensors (see tokenizer's option `return_tensors`).\n",
    "\n",
    "<font color='green'>**Question**: What is the correct way to call the tokenizer in order to obtain these results?  You can use the example provided at the end of the [DistilBertForQuestionAnswering](https://huggingface.co/transformers/model_doc/distilbert.html?distilbertforquestionanswering#distilbertforquestionanswering) documentation.  <br/>Please define a *question* and a *text* string of your own, and store the result of the tokenizer in a variable called *input*.  <br/>   Please verify (by converting back to the result) that the input has the correct tokens.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "spanish-governor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 18 tokens\n",
      "\n",
      "The original sentence recreated from the tokens:\n",
      "[CLS] What colour is that motorcycle? [SEP] That motorcycle has a special light green colour. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Question and source text\n",
    "question, text = \"What colour is that motorcycle?\", \"That motorcycle has a special light green colour.\"\n",
    "\n",
    "# Call the tokenizer on the question/text and store the result in a variable\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "print(f\"There are a total of {len(inputs[\"input_ids\"][0])} tokens\\n\")\n",
    "\n",
    "# Verification by converting the input_ids back to tokens\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "orig_sentence = tokenizer.decode(input_ids)\n",
    "print(\"The original sentence recreated from the tokens:\")\n",
    "print(orig_sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "structured-parts",
   "metadata": {},
   "source": [
    "### d. Execution of the model over the input question and text\n",
    "\n",
    "In this section, you will create an instance of the BERT neural network adapted to question answering.  The class is named `DistilBertForQuestionAnswering`.  \n",
    "\n",
    "**Important note:** The model itself (the weights) is the one that you found at the end of (1a) above which is suited for question answering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "id": "demographic-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForQuestionAnswering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "located-dayton",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please create an instance of the model here.</font>  The data will be downloaded the first time you create it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "id": "median-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ultimate-firmware",
   "metadata": {},
   "source": [
    "The results of applying the model to your question and text (i.e. extracting the answer) are obtained by calling the model with the correct inputs.  \n",
    "\n",
    "<font color='green'>**Task**: Please use the inputs you obtained above and read the [documentation of the DistilBertForQuestionAnswering class](https://huggingface.co/transformers/model_doc/distilbert.html?distilbertforquestionanswering#distilbertforquestionanswering) (under *forward*) to apply the model to your data.  Store the results in a variable called *outputs*.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "id": "norman-staff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-2.7944, -3.8852, -5.2124, -6.7933, -5.1928, -3.8815, -3.8448, -1.3391,\n",
      "          3.0379,  1.5762,  0.4520,  5.1639,  4.5670,  9.2010,  6.9972, -0.7898,\n",
      "         -2.8604, -1.3391]]), end_logits=tensor([[-0.7377, -4.3996, -3.7055, -6.3813, -6.0116, -2.9843, -4.2064, -0.9374,\n",
      "         -3.0495, -0.8349, -4.5041, -2.4779, -0.7062,  2.4648,  9.8496,  7.1742,\n",
      "          4.9999, -0.9373]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "coated-guitar",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please answer the following three questions: \n",
    "- Where are the probability values for the position of the **start** of the answer in *outputs*?\n",
    "- Are these actual probabilities or other type of coefficients?  \n",
    "- How many values are there, and is this coherent with your observations in (1b)?</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "coral-mozambique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit values for the start position: \n",
      "tensor([-2.7944, -3.8852, -5.2124, -6.7933, -5.1928, -3.8815, -3.8448, -1.3391,\n",
      "         3.0379,  1.5762,  0.4520,  5.1639,  4.5670,  9.2010,  6.9972, -0.7898,\n",
      "        -2.8604, -1.3391])\n",
      "\n",
      "torch.Size([1, 18])\n"
     ]
    }
   ],
   "source": [
    "# The probability values for the position of the start of the answer are stored in a tensor in the outputs variable.\n",
    "print(f\"Logit values for the start position: \\n{outputs[0][0]}\\n\")\n",
    "\n",
    "# These values are the start_logits and end_logits before going through the SoftMax.\n",
    "# So they do not yet represent actual probabilities, but can still be used to check which tokens the model weights the highest.\n",
    "\n",
    "# There is a total of 33 values.\n",
    "# This makes sense and there is one value for each token created from the text during tokenization.\n",
    "print(inputs.input_ids.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "horizontal-ridge",
   "metadata": {},
   "source": [
    "### e. Determination of the start and the end of the answer in the text\n",
    "\n",
    "<font color='green'>**Question**: Please use the *outputs* of the model to determine the most likely start and end of the answer span in your text, and then obtain the actual answer.  How satisfied are you with the answer?</font>  You may use help from the [ðŸ¤— Huggingface entry on question answering](https://huggingface.co/transformers/task_summary.html#extractive-question-answering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "expected-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start index: 13\n",
      "End index: 14\n",
      "Token IDs of the answer: tensor([1609, 2448])\n",
      "light green\n"
     ]
    }
   ],
   "source": [
    "# Get the index for the start end end tokens of the answer\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "# Get all the token IDs which the prediction found for the coplete answer\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "\n",
    "# Print the found start and end indices\n",
    "print(f\"Start index: {answer_start_index}\")\n",
    "print(f\"End index: {answer_end_index}\")\n",
    "\n",
    "# Print the token ids for the answer\n",
    "print(f\"Token IDs of the answer: {predict_answer_tokens}\")\n",
    "\n",
    "# Print the actual answer\n",
    "answer = tokenizer.decode(predict_answer_tokens)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "id": "2a3ca8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The answer is only the word \"special\".\n",
    "# While this is part of the answer i would expect (\"special light green\" / \"light green\"), the information about it being green is missing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "subsequent-hours",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please write a function called *answer_extraction* that gathers the previous operations: it takes two strings as arguments, creates instances of the tokenizer and the model, extracts the answer, and returns it as a string (possibly empty).  Do not create a new *tokenizer* and *model*, but assume that the ones you created above are global variables accessible from this function.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "atomic-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_extraction(question, text):\n",
    "\n",
    "    # Create the inputs\n",
    "    question.lower()\n",
    "    inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the outputs\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the index for the start end end tokens of the answer\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    # Get all the token IDs which the prediction found for the coplete answer\n",
    "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "\n",
    "    # Print the actual answer\n",
    "    answer = tokenizer.decode(predict_answer_tokens)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "approximate-samoa",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please test the function on the following questions and short text.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "martial-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many cantons are there in Switzerland?\n",
      "Answer: 26\n",
      "\n",
      "What is Switzerland famous for?\n",
      "Answer: neutrality\n",
      "\n",
      "What are the official languages of Switzerland?\n",
      "Answer: German, French, Italian, and Romansh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Excerpt from Simple English Wikipedia:\n",
    "text = \"\"\"Switzerland is a small country in Western Europe. \n",
    "Switzerland is a confederation of even smaller states, which are the 26 cantons.\n",
    "Switzerland is known for its neutrality.  Switzerland has been neutral since 1815. \n",
    "There are four official languages in Switzerland: German, French, Italian, and Romansh. \n",
    "\"\"\"\n",
    "question1 = \"How many cantons are there in Switzerland?\"\n",
    "question2 = \"What is Switzerland famous for?\"\n",
    "question3 = \"What are the official languages of Switzerland?\"\n",
    "\n",
    "# Answer question 1\n",
    "answer1 = answer_extraction(question1, text)\n",
    "print(question1)\n",
    "print(f\"Answer: {answer1}\\n\")\n",
    "\n",
    "# Answer question 2\n",
    "answer2 = answer_extraction(question2, text)\n",
    "print(question2)\n",
    "print(f\"Answer: {answer2}\\n\")                                                                \n",
    "\n",
    "# Answer question 3\n",
    "answer3 = answer_extraction(question3, text)\n",
    "print(question3)\n",
    "print(f\"Answer: {answer3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-authorization",
   "metadata": {},
   "source": [
    "## 2. Fragment retrieval using `Gensim` (from Lab 4)\n",
    "\n",
    "In this part, you will simply reuse code from Lab 4 to build a simple text retrieval system over the *Lee Corpus* provided with Gensim (300 news articles from the Australian Broadcasting Corporation).  \n",
    "* The [Gensim tutorial on topics and transformations](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py) provides the main idea.  \n",
    "* The goal is to retrieve, given a question, a short text fragment that is most likely to contain the answer.  As articles are not divided into paragraphs, you will refactor the collection of articles into a collection of fragments of at most *N* sentences each (without mixing articles). \n",
    "* The question will be used as a *query*, with the pre-processing options of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "id": "fourth-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gensim, nltk, os\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "# from TextPreprocessor import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "furnished-introduction",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Load the articles of the Lee Background Corpus provided with Gensim into a list of strings (each article in a string) called *raw_articles*.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "id": "skilled-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hundreds of people have been forced to vacate their homes in the Southern Highlands of New South Wales as strong winds today pushed a huge bushfire towards the town of Hill Top. A new blaze near Goulburn, south-west of Sydney, has forced the closure of the Hume Highway. At about 4:00pm AEDT, a marked deterioration in the weather as a storm cell moved east across the Blue Mountains forced authorities to make a decision to evacuate people from homes in outlying streets at Hill Top in the New South Wales southern highlands. An estimated 500 residents have left their homes for nearby Mittagong. The New South Wales Rural Fire Service says the weather conditions which caused the fire to burn in a finger formation have now eased and about 60 fire units in and around Hill Top are optimistic of defending all properties. As more than 100 blazes burn on New Year's Eve in New South Wales, fire crews have been called to new fire at Gunning, south of Goulburn. While few details are available at this stage, fire authorities says it has closed the Hume Highway in both directions. Meanwhile, a new fire in Sydney's west is no longer threatening properties in the Cranebrook area. Rain has fallen in some parts of the Illawarra, Sydney, the Hunter Valley and the north coast. But the Bureau of Meteorology's Claire Richards says the rain has done little to ease any of the hundred fires still burning across the state. \"The falls have been quite isolated in those areas and generally the falls have been less than about five millimetres,\" she said. \"In some places really not significant at all, less than a millimetre, so there hasn't been much relief as far as rain is concerned. \"In fact, they've probably hampered the efforts of the firefighters more because of the wind gusts that are associated with those thunderstorms.\" \n",
      "Number of strings in the list: 300\n"
     ]
    }
   ],
   "source": [
    "# Load the Lee Corpus into a list of strings\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "raw_articles = open(lee_train_file).read().splitlines()\n",
    "\n",
    "# Check the data\n",
    "print(raw_articles[0])\n",
    "print(f\"Number of strings in the list: {len(raw_articles)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "strong-collins",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please transform the articles into a collection of text fragments called *corpus1* (a list of lists of strings), by cutting each article into fragments of *N* consecutive sentences (e.g. *N* = 4), except possibly for the last fragment, and tokenizing each sentence.  At the end, display the number of fragments of your collection.</font>\n",
    "* Do not mix sentences from different articles in each fragment.\n",
    "* The reason for this operation is that full articles are too long to give to DistilBERT as texts. (Try it!)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e8e1051",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Do not forget to pre-process the articles in preparation for search -- tokenization, stopword removal, and other operations if you want to explore them.</font>  \n",
    "* A  text fragment is thus a list of strings (tokens). \n",
    "* Please inspect your corpus to make sure it is correctly built."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ead35154",
   "metadata": {},
   "source": [
    "Hint: the raw text will start with:\n",
    " \n",
    "*Hundreds of people have been forced to vacate their homes in the Southern Highlands of New South Wales as strong winds today pushed a huge bushfire towards the town of Hill Top. A new blaze near Goulburn, south-west of Sydney, has forced the closure of the Hume Highway. At about 4:00pm AEDT, a marked deterioration in the weather as a storm cell moved east across the Blue Mountains forced authorities to make a decision to evacuate people from homes in outlying streets at Hill Top in the New South Wales southern highlands. An estimated 500 residents have left their homes for nearby Mittagong. The New South Wales Rural Fire Service says the weather conditions which caused the fire to burn in a finger formation have now eased and about 60 fire units in and around Hill Top are optimistic of defending all properties. As more than 100 blazes burn on New Year's Eve in New South Wales, fire crews have been called to new fire at Gunning, south of Goulburn. While few details are available at this stage, fire authorities says it has closed the Hume Highway in both directions. Meanwhile, a new fire in Sydney's west is no longer threatening properties in the Cranebrook area. ....*\n",
    "\n",
    "The first 8 sentences are to be decomposed into:\n",
    "\n",
    "*[['hundreds', 'people', 'forced', 'vacate', 'homes', 'southern', 'highlands', 'new', 'south', 'wales', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'town', 'hill', 'top', '.', 'new', 'blaze', 'near', 'goulburn', ',', 'south-west', 'sydney', ',', 'forced', 'closure', 'hume', 'highway', '.', '4:00pm', 'aedt', ',', 'marked', 'deterioration', 'weather', 'storm', 'cell', 'moved', 'east', 'across', 'blue', 'mountains', 'forced', 'authorities', 'make', 'decision', 'evacuate', 'people', 'homes', 'outlying', 'streets', 'hill', 'top', 'new', 'south', 'wales', 'southern', 'highlands', '.', 'estimated', '500', 'residents', 'left', 'homes', 'nearby', 'mittagong', '.'], ['new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'weather', 'conditions', 'caused', 'fire', 'burn', 'finger', 'formation', 'eased', '60', 'fire', 'units', 'around', 'hill', 'top', 'optimistic', 'defending', 'properties', '.', '100', 'blazes', 'burn', 'new', 'year', 'eve', 'new', 'south', 'wales', ',', 'fire', 'crews', 'called', 'new', 'fire', 'gunning', ',', 'south', 'goulburn', '.', 'details', 'available', 'stage', ',', 'fire', 'authorities', 'says', 'closed', 'hume', 'highway', 'directions', '.', 'meanwhile', ',', 'new', 'fire', 'sydney', 'west', 'longer', 'threatening', 'properties', 'cranebrook', 'area', '.']*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e1d182e",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: If possible, store the original version of each fragment into a string in *corpus2* (i.e. non-tokenized, non-lowercased, etc.), because it will be better to pass it later to DistilBERT.  Otherwise, you can also reconstruct the full fragment from the tokens in corpus1.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "id": "bc0d8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Imstantiate tokenizer that can remove punctuation\n",
    "tokenizer_regexp = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Number of sentences per sublist\n",
    "N = 4\n",
    "\n",
    "# Define empty lists to store the fragments\n",
    "corpus1 = []\n",
    "corpus2 = []\n",
    "\n",
    "# Stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "# Add additional characters to the just created set of stop words\n",
    "for sw in ['\\\"', '\\'', '\\'\\'', '`', '``', '\\'s']:\n",
    "    stop_words.append(sw)\n",
    "\n",
    "# Loop through each raw article\n",
    "for raw_article in raw_articles:\n",
    "    # Tokenize the article into sentences\n",
    "    sentences = nltk.tokenize.sent_tokenize(raw_article)\n",
    "    i = 0\n",
    "    # Process each group of N sentences\n",
    "    while i < len(sentences):\n",
    "        # Concatenate N sentences to form a document\n",
    "        document = \" \".join(sentences[i:i+N])\n",
    "        # Append the document to corpus2\n",
    "        corpus2.append(document)\n",
    "        # Tokenize the document into words and preprocess\n",
    "        # tokens = nltk.word_tokenize(document)\n",
    "        tokens = tokenizer_regexp.tokenize(document)\n",
    "\n",
    "        tokens = [w.lower() for w in tokens if w not in stop_words]\n",
    "        # Append the preprocessed tokens to corpus1\n",
    "        corpus1.append(tokens)\n",
    "        # Move to the next group of sentences\n",
    "        i += N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "id": "6c19eb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text data:\n",
      " Hundreds of people have been forced to vacate their homes in the Southern Highlands of New South Wales as strong winds today pushed a huge bushfire towards the town of Hill Top. A new blaze near Goulburn, south-west of Sydney, has forced the closure of the Hume Highway. At about 4:00pm AEDT, a marked deterioration in the weather as a storm cell moved east across the Blue Mountains forced authorities to make a decision to evacuate people from homes in outlying streets at Hill Top in the New South Wales southern highlands. An estimated 500 residents have left their homes for nearby Mittagong.\n",
      "\n",
      "Processed text data:\n",
      " ['hundreds', 'people', 'forced', 'vacate', 'homes', 'southern', 'highlands', 'new', 'south', 'wales', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'town', 'hill', 'top', 'a', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'sydney', 'forced', 'closure', 'hume', 'highway', 'at', '4', '00pm', 'aedt', 'marked', 'deterioration', 'weather', 'storm', 'cell', 'moved', 'east', 'across', 'blue', 'mountains', 'forced', 'authorities', 'make', 'decision', 'evacuate', 'people', 'homes', 'outlying', 'streets', 'hill', 'top', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', '500', 'residents', 'left', 'homes', 'nearby', 'mittagong']\n",
      "\n",
      "Number of fragments:\n",
      " 790\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the raw and processed text data as well as the overall number of fragments\n",
    "print(f\"Raw text data:\\n {corpus2[0]}\\n\")\n",
    "print(f\"Processed text data:\\n {corpus1[0]}\\n\")\n",
    "print(f\"Number of fragments:\\n {len(corpus1)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "everyday-canadian",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please create a search index (called *search_index*) using a *tfidf* model and transform all text fragments from *corpus1* into document vectors.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "id": "b58c315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MatrixSimilarity<790 docs, 7125 features>\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary object to map words to unique integer IDs\n",
    "dictionary = gensim.corpora.Dictionary(corpus1)\n",
    "\n",
    "# Convert each document in the corpus into a bag-of-words representation\n",
    "# This representation is a list of (word_id, word_frequency) tuples for each document\n",
    "corpus = [dictionary.doc2bow(text) for text in corpus1]\n",
    "\n",
    "# step 1 -- initialize a model\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "# Create a TF-IDF model based on the input corpus of documents.\n",
    "# This model will be used to transform the original bag-of-words representation of documents into TF-IDF weighted vectors,\n",
    "# where the weights represent the importance of each word in each document relative to the entire corpus.\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# Calculate the similarities of the documents \n",
    "search_index = gensim.similarities.MatrixSimilarity(corpus_tfidf, num_features=len(dictionary))\n",
    "print(search_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "certified-pitch",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please write a function called *fragment_retrieval* which returns the most relevant text fragment (string) from the corpus given a question, which is used as the query.</font>  \n",
    "* The function processes the query in the same way as the documents (using the *tfidf model*) to obtain a *vectorized_query*.\n",
    "* This is passed to the *search_index* to rank all documents by relevance.\n",
    "* All the resources created above are supposed available as global variables (the dictionary, the tfidf model, the search_index, the corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "id": "disturbed-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def fragment_retrieval(query):\n",
    "\n",
    "    # Transform the query into TF-IDF representation\n",
    "    query = query.translate(str.maketrans ('', '', string.punctuation)) # Remove punctuation\n",
    "    query_words = query.lower().split()\n",
    "    query_bow = dictionary.doc2bow(query_words)\n",
    "    query_tfidf = tfidf[query_bow]\n",
    "\n",
    "    # Calculate similarity scores between the query and all documents by comparing their tf-idf vectors\n",
    "    sims = search_index[query_tfidf]\n",
    "\n",
    "    # Sort the similarity scores and retrieve the indices of the most similar (n) documents\n",
    "    n = 1\n",
    "    top_similar_indices = sorted(enumerate(sims), key=lambda x: -x[1])[:n]\n",
    "\n",
    "    # Retrieve the fragment based on the indices obtained\n",
    "    top_similar_fragments = [corpus2[index[0]] for index in top_similar_indices]\n",
    "\n",
    "    # Combine the fragments into whole sentences\n",
    "    fragments_whole = [\"\".join([str(item) for item in fragment]) for fragment in top_similar_fragments]\n",
    "\n",
    "    # Return the most similar fragments\n",
    "    return fragments_whole\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "agricultural-rapid",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please apply the above function to the three queries provided below.</font>  \n",
    "\n",
    "Note: again, the corpus, search_index, tfidf and dictionary are available as global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "id": "automotive-recognition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the mayor of New York? -> \"I felt that my job as the mayor was to turn around the city, because I believed - rightly or wrongly - that we had one last chance to do that.\" Mr Giuliani, a Republican, has served two terms as New York City's Mayor since 1993. Term limits prevent him from seeking a third term in office, and he will be succeeded by billionaire media mogul Michael Bloomberg.\n",
      "Who is Nicole Kidman? -> In the United States, Australian actress Nicole Kidman has been nominated for two Golden Globe best actor awards for her roles in the Australian-made musical \"Moulin Rouge\", and in her new thriller \"The Others\". \"Moulin Rouge\" also is one of two pictures leading the Golden Globe nominations, with six possible awards. It is vying for best musical or comedy picture of 2002, best actress in a comedy or musical, best actor in the same category for Ewen McGregor, best director for Baz Luhrmann, best original score and best original song. The other film to pick up six nominations is the Ron Howard directed \"A Beautiful Mind\" starring Oscar winner Russell Crowe.\n",
      "How many Australians died in the 1999 Interlaken canyoning accident? -> Eight people are to appear in a Swiss court tomorrow charged with the manslaughter of 18 tourists and three guides, after the 1999 Interlaken canyoning tragedy. The first three defendants are managers of the now defunctoperator, Adventure World. Twenty-one people including 14 Australians were killed when a thunderstorm struck when they were canyoning down the Saxeten River Gorge near Interlaken. A massive wall of water hit the group and swept them to their deaths.\n"
     ]
    }
   ],
   "source": [
    "queries = [\"Who is the mayor of New York?\", \n",
    "           \"Who is Nicole Kidman?\",\n",
    "           \"How many Australians died in the 1999 Interlaken canyoning accident?\"]\n",
    "for q in queries:\n",
    "    print(q, '->', fragment_retrieval(q)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "closed-clause",
   "metadata": {},
   "source": [
    "## 3. Integration, testing and discussion\n",
    "\n",
    "<font color='green'>**Task**: Using the two functions 'fragment_retrieval' and 'answer_extraction' from parts 1 and 2, and assuming all models and data are available as global variables, please create a unique function which returns the answer (string) to a question (string).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "id": "metallic-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answering(question):\n",
    "    fragment = fragment_retrieval(question)\n",
    "    print(\"\")\n",
    "    print(f\"Fragment used for context: {fragment[0]}\")\n",
    "    answer = answer_extraction(question, fragment[0])\n",
    "    return(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "purple-impact",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please add between 5 and 10 more questions to the following list.  You can add answerable and non-answerable questions (with respect to the corpus).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "id": "suburban-commercial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fragment used for context: \"I felt that my job as the mayor was to turn around the city, because I believed - rightly or wrongly - that we had one last chance to do that.\" Mr Giuliani, a Republican, has served two terms as New York City's Mayor since 1993. Term limits prevent him from seeking a third term in office, and he will be succeeded by billionaire media mogul Michael Bloomberg.\n",
      "Who is the mayor of New York? -> Mr Giuliani\n",
      "\n",
      "Fragment used for context: In the United States, Australian actress Nicole Kidman has been nominated for two Golden Globe best actor awards for her roles in the Australian-made musical \"Moulin Rouge\", and in her new thriller \"The Others\". \"Moulin Rouge\" also is one of two pictures leading the Golden Globe nominations, with six possible awards. It is vying for best musical or comedy picture of 2002, best actress in a comedy or musical, best actor in the same category for Ewen McGregor, best director for Baz Luhrmann, best original score and best original song. The other film to pick up six nominations is the Ron Howard directed \"A Beautiful Mind\" starring Oscar winner Russell Crowe.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Nicole Kidman? -> Australian actress\n",
      "\n",
      "Fragment used for context: Eight people are to appear in a Swiss court tomorrow charged with the manslaughter of 18 tourists and three guides, after the 1999 Interlaken canyoning tragedy. The first three defendants are managers of the now defunctoperator, Adventure World. Twenty-one people including 14 Australians were killed when a thunderstorm struck when they were canyoning down the Saxeten River Gorge near Interlaken. A massive wall of water hit the group and swept them to their deaths.\n",
      "How many Australians died in the 1999 Interlaken canyoning accident? -> 14\n",
      "\n",
      "Fragment used for context: Eight people are to appear in a Swiss court tomorrow charged with the manslaughter of 18 tourists and three guides, after the 1999 Interlaken canyoning tragedy. The first three defendants are managers of the now defunctoperator, Adventure World. Twenty-one people including 14 Australians were killed when a thunderstorm struck when they were canyoning down the Saxeten River Gorge near Interlaken. A massive wall of water hit the group and swept them to their deaths.\n",
      "What caused the 1999 Interlaken canyoning accident? -> thunderstorm\n",
      "\n",
      "Fragment used for context: \"Of course fundamental principles of fairness require that before that stage of the inquiry is reached, any person whose interest might be adversely affected by a finding of a fact or a recommendation must be made clearly and unequivocally aware of that risk and be given ample opportunity to present any evidence and submissions relevant to that issue,\" he said.\n",
      "Which city is the capital of Australia? -> Australia\n",
      "\n",
      "Fragment used for context: \"Of course fundamental principles of fairness require that before that stage of the inquiry is reached, any person whose interest might be adversely affected by a finding of a fact or a recommendation must be made clearly and unequivocally aware of that risk and be given ample opportunity to present any evidence and submissions relevant to that issue,\" he said.\n",
      "Who is the prime-minister of Israel? -> \n",
      "\n",
      "Fragment used for context: An initial fleet of four Boeing 767-300 aircraft will eventually be increased to 12 and services will be extended to every Australian mainland capital, including Perth and Darwin. Australian Airlines is currently negotiating with various unions on wages, conditions and work practices. Australian Airlines is expected to announce it will base its operations in Cairns, following yesterday's Queensland Cabinet approval of an incentive package for the airline. The new airline plans to cut costs by outsourcing some maintenance work and reducing the number of flight attendants.\n",
      "What are the main Australian airlines? -> Perth and Darwin\n",
      "\n",
      "Fragment used for context: Elka Graham won the women's 300 metres freestyle which was contested by having three races over 100 metres. She says former champion Kieren Perkins was behind her victory. \"I swam with Kieren this morning and he gave me some awesome advice. He told me to close my eyes [in the last 25 metres] and to absolutely go for it,\" she said.\n",
      "What is Kieren Perkins' sport? -> swam\n"
     ]
    }
   ],
   "source": [
    "questions = [\"Who is the mayor of New York?\", \n",
    "            \"Who is Nicole Kidman?\", \n",
    "            \"How many Australians died in the 1999 Interlaken canyoning accident?\",\n",
    "            \"What caused the 1999 Interlaken canyoning accident?\",\n",
    "            \"Which city is the capital of Australia?\",\n",
    "            \"Who is the prime-minister of Israel?\",\n",
    "            \"What are the main Australian airlines?\",\n",
    "            \"What is Kieren Perkins' sport?\"]\n",
    "for q in questions:\n",
    "    print(q, '->', question_answering(q))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fabulous-active",
   "metadata": {},
   "source": [
    "<font color='green'>**Task**: Please discuss the correctness of the answers, give possible reasons for incorrect ones, and make suggestions for improvements.</font>\n",
    "\n",
    "Write your discussion here or in a cell below.\n",
    "\n",
    "When you have finished please clean and re-run one last time the notebook, from start to end, then submit it on Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6010223d",
   "metadata": {},
   "source": [
    "<font color='lightblue'> About 50 % of the answers seemed to be correct at first. <br>\n",
    " When looking at the question \"Who is Nicole Kidman?\", it seems that the \"?\" lead to a different fragment, which doesn't have anything to do with the actress. Removing punctuations before tokenizing the queries solved this problem. <br>\n",
    " For the question regarding the prime-minister of Israel, the \"-\" in prime-minister seems to be the main cause, since the question gets answered correctly if it is written as \"prime minister\". This could be improved by handling these types of words better during tokenization. <br>\n",
    " The question regarding the main Australian airlines might struggle with the fact, that the context mentions Perth and Darwin in the context of \"mainland capitals\". This might have less to do with tokenization and more with the context being harder to interpret for DistilBERT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msenlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
