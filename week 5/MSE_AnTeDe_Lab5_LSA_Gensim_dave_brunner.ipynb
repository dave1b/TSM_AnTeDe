{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MSE Logo](https://moodle.msengineering.ch/pluginfile.php/1/core_admin/logocompact/300x300/1613732714/logo-mse.png \"MSE Logo\") \n",
    "\n",
    "# AnTeDe Lab 5: Latent Semantic Analysis with Gensim\n",
    "\n",
    "## Objective\n",
    "The goal of this lab is to perform LSA on a small corpus of news.  You will use the LSA word vectors to estimate word similarity, and then to perform ranked retrieval given a query. \n",
    "\n",
    "<font color='green'>Please answer the questions in green within this notebook, and submit the completed notebook under the corresponding homework on Moodle.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:08.633698Z",
     "start_time": "2024-03-25T16:04:07.730138Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "from gensim import models, corpora, similarities\n",
    "from gensim.models import LsiModel, LdaModel, LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:08.670113Z",
     "start_time": "2024-03-25T16:04:08.634739Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import TextProcessor.py from local directory structure\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../week 1'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from TextPreprocessor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used in this lab the same set of 300 Australian that you used in Lab 4 on Document Representation.  It is a shortened version of the Lee Background Corpus [described here](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF) and it is available with the **gensim** package that you installed.  The following code will load the documents into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:08.672944Z",
     "start_time": "2024-03-25T16:04:08.670635Z"
    }
   },
   "outputs": [],
   "source": [
    "# Code inspired from:\n",
    "# https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb\n",
    "\n",
    "test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_train_file = test_data_dir + os.sep + 'lee_background.cor'\n",
    "text = open(lee_train_file).read().splitlines()\n",
    "data_df = pd.DataFrame({'text': text})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "You will need first to preprocess the data through the following stages:\n",
    "1. tokenization\n",
    "2. stopword removal\n",
    "2. POS-based filtering (optional)\n",
    "3. lemmatization or stemming (optional)\n",
    "4. addition of bigrams to each document (optional)\n",
    "5. filtering of infrequent words\n",
    "6. inspection and filtering of frequent words\n",
    "\n",
    "Use our in-house `TextPreprocessor.py` file, as explained in Lab 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing stages / steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:09.009425Z",
     "start_time": "2024-03-25T16:04:08.674075Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/davebrunner/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:09.014455Z",
     "start_time": "2024-03-25T16:04:09.010877Z"
    }
   },
   "outputs": [],
   "source": [
    "# Please write here the preprocessing instructions\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.update(\n",
    "    [',', '.', '!', '?', ';', ':', '(', ')', '[', ']', '{', '}', '``', \"''\", '\"\"', '``', \"''\", '\"\"', '’', '“', '”', '‘',\n",
    "     '—', '–', '…', '•', '·', '°', '€', '£', '¥', '¢', '§', '©', '®', '™', '¶', '†', '‡', '‰', '№', 'Ω', '℮', '→', '↔'])\n",
    "processor = TextPreprocessor(stopwords=stopwords, lemmatize=True, stem=False, min_length=2, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:11.612929Z",
     "start_time": "2024-03-25T16:04:09.015326Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davebrunner/.local/share/virtualenvs/TSM_AnTeDe-RXa4k3O7/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "data_df['processed'] = processor.transform(data_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:11.674323Z",
     "start_time": "2024-03-25T16:04:11.613927Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df['tokenized'] = data_df['processed'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:11.677107Z",
     "start_time": "2024-03-25T16:04:11.675257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['union', 'represent', 'qantas', 'maintenance', 'worker', 'warn', 'escalate', 'industrial', 'action', 'company', 'reject', 'offer', 'long', 'run', 'dispute', 'arbitrate', 'party', 'lock', 'private', 'talk', 'yesterday', 'industrial', 'relation', 'commission', '3,000', 'maintenance', 'worker', 'earlier', 'vote', 'reject', 'qantas', 'propose', 'wage', 'freeze', 'national', 'secretary', 'australian', 'manufacturing', 'worker', 'union', 'amwu', 'doug', 'cameron', 'say', 'union', 'do', 'everything', 'possible', 'resolve', 'dispute', 'qantas', 'prepared', 'accept', 'private', 'arbitration', 'absolutely', 'alternative', 'worker', 'take', 'industrial', 'action', 'escalate', 'industrial', 'action', 'necessary', 'ensure', 'get', 'fair', 'company', 'seem', 'determine', 'crush', 'underfoot', 'say']\n"
     ]
    }
   ],
   "source": [
    "print(data_df['tokenized'].iloc[120])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make a list of all words from all articles.  Then, using `nltk.FreqDist`, consider the most frequent and the least frequent words.  If you find uninformative words among the most frequent ones, please remove them from the articles.  Similarly, please remove from articles the words appearing fewer than 2 or 3 times in the corpus.  \n",
    "\n",
    "<font color='green'> **Question**:  Please justify these choices. What is now the size of your vocabulary?</font> "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most commen: [('say', 1011), ('australian', 178), ('new', 171), ('palestinian', 168), ('australia', 157), ('people', 153), ('government', 146), ('attack', 140), ('two', 136), ('day', 131), ('south', 130), ('state', 129), ('force', 126), ('year', 126), ('would', 115), ('take', 114), ('one', 114), ('israeli', 112), ('also', 111), ('minister', 106), ('fire', 103), ('last', 102), ('first', 102), ('arafat', 96), ('make', 92), ('afghanistan', 90), ('united', 89), ('three', 87), ('police', 86), ('world', 84), ('security', 83), ('time', 83), ('official', 83), ('could', 82), ('report', 81), ('call', 80), ('kill', 80), ('area', 79), ('give', 78), ('today', 77), ('leader', 77), ('group', 75), ('told', 75), ('come', 74), ('get', 74), ('company', 73), ('union', 71), ('authority', 69), ('laden', 69), ('well', 68)]\n",
      "------------------\n",
      "Most uncommen: [('check-in', 1), ('terminate', 1), ('throw', 1), ('basically', 1), ('unwind', 1), ('recrimination', 1), ('congressman', 1), ('excess', 1), ('precursor', 1), ('inject', 1), ('mouse', 1), ('tissue', 1), ('variety', 1), ('parkinson', 1), ('world-wide', 1), ('epidemic', 1), ('annually', 1), ('ukraine', 1), ('insecurity', 1), ('steep', 1), ('merger', 1), ('stone', 1), ('muscle', 1), ('grouping', 1), ('merge', 1), ('categorically', 1), ('academic', 1), ('centenary', 1), ('recognises', 1), ('1901.', 1), ('lecturer', 1), ('bedeharris', 1), ('monarchy', 1), ('codification', 1), ('opporunity', 1), ('consult', 1), ('1-1.', 1), ('cedric', 1), ('pioline', 1), ('fabrice', 1), ('santoro', 1), ('line-up', 1), ('appraisal', 1), ('pro', 1), ('con', 1), ('salvage', 1), ('overcame', 1), ('sebastien', 1), ('grosjean', 1), ('vice-versa', 1)]\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for doc in data_df['tokenized'] for word in doc]\n",
    "fdist = nltk.FreqDist(all_words)\n",
    "print(f\"Most commen: {fdist.most_common(50)}\")\n",
    "print(\"------------------\")\n",
    "print(f\"Most uncommen: {fdist.most_common()[-50:]}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:11.687189Z",
     "start_time": "2024-03-25T16:04:11.677997Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# To remove uninformative words, I set min length of 2 on the TextPreprocessor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:11.690280Z",
     "start_time": "2024-03-25T16:04:11.688957Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_df['filtered'] = data_df['tokenized'].apply(lambda x: [word for word in x if fdist[word] > 2])\n",
    "all_words_filtered = [word for doc in data_df['filtered'] for word in doc]\n",
    "assert all_words.__sizeof__() > all_words_filtered.__sizeof__()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:11.694935Z",
     "start_time": "2024-03-25T16:04:11.690870Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:11.696789Z",
     "start_time": "2024-03-25T16:04:11.695454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'south', 'wale', 'firefighter', 'hop', 'wind', 'help', 'ease', 'workload', 'today', 'predict', 'nasty', 'condition', 'weekend', 'wind', 'expect', 'ease', 'today', 'weather', 'bureau', 'say', 'temperature', 'high', 'fire', 'still', 'burning', 'across', 'new', 'south', 'wale', 'rural', 'fire', 'service', 'say', 'change', 'may', 'allow', 'concentrate', 'action', 'room', 'complacency', 'mark', 'sullivan', 'rural', 'fire', 'service', 'say', 'condition', 'may', 'little', 'today', 'outlook', 'weekend', 'certainly', 'appear', 'weather', 'forecast', 'high', 'temperature', 'high', 'wind', 'certainly', 'could', 'nasty', 'couple', 'day', 'ahead', 'sullivan', 'say', 'one', 'area', 'cause', 'great', 'concern', 'today', 'long', 'blaze', 'low', 'blue', 'mountain', 'firefighter', 'also', 'keep', 'close', 'eye', 'blaze', 'spencer', 'north', 'sydney', 'yesterday', 'broke', 'containment', 'line', 'concern', 'fire', 'may', 'hawkesbury', 'river', 'continue', 'state', 'central', 'west', 'south', 'sydney', 'shoalhaven', 'illawarra', 'firefighter', 'able', 'carry', 'back-burning', 'operation', 'three', 'area', 'operation', 'carry', 'part', 'well', 'area', 'appin', 'road', 'old', 'highway', 'area', 'west', 'near', 'also', 'target', 'meanwhile', 'illawarra', 'police', 'arrest', 'three', 'teenager', 'relation', 'south', 'coast', 'new', 'south', 'wale', 'spokesman', 'say', 'three', 'small', 'fire', 'around', 'aedt', 'yesterday', 'short', 'time', 'later', 'police', 'arrest', 'three', '15-year-old', 'boy', 'height', 'three', 'interviewed', 'charge']\n"
     ]
    }
   ],
   "source": [
    "print(data_df['filtered'].iloc[33])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for (word, freq) in [(word, freq) for (word, freq) in fdist.most_common() if freq < 3]:\n",
    "    assert all_words_filtered.count(word) == 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:12.244885Z",
     "start_time": "2024-03-25T16:04:11.697436Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA with Gensim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will write the Gensim commands to compute a term-document matrix from the above documents, then transform it using SVD, and truncate the result.  To learn what the commands are, please follow the [Topics and Tranformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) from Gensim. \n",
    "\n",
    "<font color=\"green\"> **Question**: Please gather these commands into a function called `train_lsa`.  They should cover: dictionary creation, corpus mapping, computation of TF-IDF values, and creation of the LSA model.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:12.247597Z",
     "start_time": "2024-03-25T16:04:12.245470Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_lsa(filtered_texts, num_topics=10):\n",
    "    dictionary = corpora.Dictionary(filtered_texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in filtered_texts]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lsa_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "\n",
    "    return lsa_model, dictionary, corpus, corpus_tfidf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"> **Question**: Please fix the number of topics to 10.  Then, execute the cell that performs `train_lsa`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:12.251667Z",
     "start_time": "2024-03-25T16:04:12.248514Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:13.095167Z",
     "start_time": "2024-03-25T16:04:12.252206Z"
    }
   },
   "outputs": [],
   "source": [
    "lsa_model, dictionary, corpus, corpus_tfidf = train_lsa(data_df['filtered'], number_of_topics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"> **Question**: Please display several topics found by LSA using the Gensim `print_topics` function.  Please explain in your own words the meaning of what is displayed.  How do you relate it with what was explained in the course on LSA?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:13.111905Z",
     "start_time": "2024-03-25T16:04:13.106201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[(0,\n  '0.311*\"palestinian\" + 0.205*\"israeli\" + 0.185*\"arafat\" + 0.117*\"israel\" + 0.113*\"hamas\" + 0.111*\"attack\" + 0.095*\"force\" + 0.094*\"afghanistan\" + 0.094*\"gaza\" + 0.089*\"security\"'),\n (1,\n  '0.426*\"palestinian\" + 0.281*\"israeli\" + 0.255*\"arafat\" + 0.158*\"israel\" + 0.153*\"hamas\" + 0.132*\"gaza\" + 0.114*\"sharon\" + 0.101*\"suicide\" + -0.101*\"afghanistan\" + -0.091*\"south\"'),\n (2,\n  '-0.251*\"qantas\" + 0.220*\"afghanistan\" + 0.208*\"bin\" + 0.205*\"laden\" + -0.202*\"worker\" + -0.192*\"union\" + 0.161*\"qaeda\" + -0.142*\"industrial\" + 0.141*\"bora\" + 0.141*\"tora\"'),\n (3,\n  '0.303*\"qantas\" + -0.259*\"test\" + 0.240*\"worker\" + 0.223*\"union\" + 0.170*\"industrial\" + -0.167*\"south\" + -0.158*\"africa\" + 0.143*\"maintenance\" + -0.126*\"waugh\" + -0.124*\"match\"'),\n (4,\n  '0.231*\"fire\" + -0.207*\"test\" + -0.131*\"africa\" + -0.127*\"qantas\" + -0.118*\"afghanistan\" + 0.117*\"river\" + 0.116*\"firefighter\" + 0.115*\"wind\" + 0.113*\"sydney\" + -0.112*\"bin\"'),\n (5,\n  '-0.249*\"fire\" + 0.193*\"guide\" + 0.190*\"river\" + 0.183*\"canyoning\" + 0.172*\"adventure\" + 0.157*\"trip\" + 0.145*\"friedli\" + 0.142*\"court\" + -0.139*\"firefighter\" + 0.138*\"interlaken\"'),\n (6,\n  '0.187*\"qantas\" + -0.167*\"rate\" + -0.135*\"economy\" + 0.135*\"river\" + -0.123*\"government\" + 0.121*\"worker\" + 0.112*\"guide\" + -0.109*\"india\" + -0.108*\"asylum\" + -0.106*\"australia\"'),\n (7,\n  '0.277*\"rate\" + -0.235*\"detainee\" + 0.201*\"bank\" + -0.187*\"woomera\" + 0.166*\"economy\" + 0.164*\"cut\" + -0.154*\"centre\" + -0.145*\"asylum\" + 0.145*\"reserve\" + 0.137*\"per\"'),\n (8,\n  '0.247*\"commission\" + 0.186*\"hih\" + 0.185*\"royal\" + 0.170*\"hollingworth\" + -0.145*\"qantas\" + 0.125*\"evidence\" + 0.123*\"allegation\" + 0.119*\"collapse\" + 0.116*\"hearing\" + 0.116*\"abuse\"'),\n (9,\n  '0.272*\"india\" + 0.175*\"indian\" + 0.135*\"pakistan\" + -0.132*\"afghanistan\" + 0.132*\"man\" + -0.125*\"commission\" + -0.125*\"yacht\" + -0.119*\"metre\" + -0.116*\"interim\" + 0.115*\"attack\"')]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n",
    "The print_topics function displays the top words for each topic. The first number in each tuple is the weight of the word in the topic, and the second number is the word itself"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"> **Question**: Please define a function that returns the cosine similarity between two words (testing first if they are in the vocabulary). Please exemplify its value on two different word pairs, one of which should be obviously more similar than the other, and comment the values.</font>  You can get inspiration from this [Gensim Tutorial on Document Similarity](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:13.116671Z",
     "start_time": "2024-03-25T16:04:13.113428Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordsim(word1, word2, model, dictionary_):\n",
    "    vec1 = model[dictionary_.doc2bow([word1])]\n",
    "    vec2 = model[dictionary_.doc2bow([word2])]\n",
    "    return gensim.matutils.cossim(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game and wale -> 0.21850186325286072\n",
      "Attack and war -> 0.6767999008799849\n"
     ]
    }
   ],
   "source": [
    "print(f\"Game and wale -> {wordsim(\"game\", \"wale\", lsa_model, dictionary)}\")\n",
    "print(f\"Attack and war -> {wordsim(\"attack\", \"war\", lsa_model, dictionary)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:13.122701Z",
     "start_time": "2024-03-25T16:04:13.118052Z"
    }
   },
   "execution_count": 19
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"> **Question**: Please use the [Gensim Tutorial on Document Similarity](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html) to write a function that prints a list of words sorted by decreasing LSA similarity with a given word and showing the score too.  You don't have to use the cosine_similarity function here.  Please choose a \"query\" word and ten other words, apply your function, and comment the results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:13.126383Z",
     "start_time": "2024-03-25T16:04:13.124146Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_ranking(word_0, word_list_, model, dictionary_):\n",
    "    sims = []\n",
    "    for word in word_list_:\n",
    "        sims.append((word, wordsim(word, word_0, model, dictionary_)))\n",
    "    return sorted(sims, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:13.146192Z",
     "start_time": "2024-03-25T16:04:13.127902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('kill', 0.7641262489704287),\n ('attack', 0.5388384066386244),\n ('war', 0.3894481351982706),\n ('defend', 0.21424124862003135),\n ('fight', 0.045305839263935936),\n ('hate', 0.0),\n ('game', -0.06611927564231743),\n ('love', -0.1140072071227226)]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word0 = \"rocket\"\n",
    "word_list = [\"war\", \"love\", \"hate\", \"game\", \"attack\", \"defend\", \"fight\", \"kill\"]\n",
    "word_ranking(word0, word_list, lsa_model, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The listing makes sense, as the word rocket is more similar to war and attack than to love and game"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\"> **Question**: Please select now a significantly larger number of topics, and train a new LSA model.  Perform the same `word_ranking` task as above and compare the new ranking with the previous one.  Which one seems better?</font>"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('kill', 0.7616049117685081),\n ('attack', 0.3424902483484021),\n ('war', 0.19689796118512962),\n ('defend', 0.11185859214100352),\n ('hate', 0.0),\n ('fight', -0.019251714677282705),\n ('game', -0.03188218739950623),\n ('love', -0.03214056290640063)]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 20\n",
    "lsa_model, dictionary, corpus, corpus_tfidf = train_lsa(data_df['filtered'], number_of_topics)\n",
    "word_ranking(word0, word_list, lsa_model, dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:13.703145Z",
     "start_time": "2024-03-25T16:04:13.147682Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('defend', 0.1410294627718505),\n ('kill', 0.08478136583807817),\n ('attack', 0.06979602070601751),\n ('love', 0.02934590177466269),\n ('war', 0.027143178786544358),\n ('hate', 0.0),\n ('fight', -0.020421597123538845),\n ('game', -0.022904489906125022)]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 100\n",
    "lsa_model, dictionary, corpus, corpus_tfidf = train_lsa(data_df['filtered'], number_of_topics)\n",
    "word_ranking(word0, word_list, lsa_model, dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:14.892267Z",
     "start_time": "2024-03-25T16:04:13.704849Z"
    }
   },
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('kill', 0.048394945314964286),\n ('fight', 0.01062061079054211),\n ('hate', 0.0),\n ('game', -0.003994180377152921),\n ('love', -0.007752508416960653),\n ('defend', -0.07083423555909896),\n ('war', -0.07362298578528105),\n ('attack', -0.19020624280406517)]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 500\n",
    "lsa_model, dictionary, corpus, corpus_tfidf = train_lsa(data_df['filtered'], number_of_topics)\n",
    "word_ranking(word0, word_list, lsa_model, dictionary)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-25T16:04:18.330380Z",
     "start_time": "2024-03-25T16:04:14.896040Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "For me it seems, that the ranking is better with a lower number of topics. The ranking with 10 topics seems to be the best."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Lab 5\n",
    "Please make sure all cells have been executed, save this completed notebook, compress it to a *zip* file, and upload it to [Moodle](https://moodle.msengineering.ch/course/view.php?id=1869)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msenlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
