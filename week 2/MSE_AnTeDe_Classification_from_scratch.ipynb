{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn5KkFo9pEgZ"
   },
   "source": [
    "# AnTeDe: Text Classification - Part A\n",
    "\n",
    "## Session goal\n",
    "The goal of this session is to implement a Multinomial Naive Bayes classifier from scratch.\n",
    "\n",
    "## Data collection\n",
    "We are going to use a small toy dataset. Each document is a single sentence. The training data contains three documents, each from a different class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDS9ibnNpEga",
    "outputId": "26a42b83-c40c-48da-ff5d-fa1d9e7c98cd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# these 3 lines are here for compatibility purposes\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "#\n",
    "\n",
    "training_corpus=[\"The Limmat flows out of the lake.\", \n",
    "           \"The bears are in the bear pit near the river.\",\n",
    "           \"The Rhône flows out of Lake Geneva.\",\n",
    "          ]\n",
    "training_labels=[\"zurich\", \n",
    "         \"bern\",\n",
    "         \"geneva\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxjVA_eWpEgc"
   },
   "source": [
    "We are also going to need a helper function that can normalize a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "3R0K6tSfpEgd"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def normalize(document, keep_punctuation=False, keep_stop_words=False,\n",
    "              keep_inflected=True, keep_numbers=False):\n",
    "    \"\"\"\n",
    "    This function normalizes the input document by tokenizing, lowercasing,\n",
    "    removing punctuation and stopwords, and lemmatizing words.\n",
    "\n",
    "    :param document: str, the input document to be normalized\n",
    "    :param keep_punctuation: bool, whether to keep punctuation in the output\n",
    "    :param keep_stop_words: bool, whether to keep stopwords in the output\n",
    "    :param keep_inflected: bool, whether to keep inflected forms of words or lemmatize them\n",
    "    :param keep_numbers: bool, whether to keep numbers in the output\n",
    "    :return: list, the normalized tokens of the input document\n",
    "    \"\"\"\n",
    "    word_tokens = word_tokenize(document)\n",
    "    wl = WordNetLemmatizer()\n",
    "    lemmatize = lambda tokens: [wl.lemmatize(w) for w in tokens]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    normalized = [w.lower() for w in word_tokens\n",
    "                  if ((not w.lower() in set(string.punctuation)) or keep_punctuation)\n",
    "                  and ((not w.lower() in stop_words) or keep_stop_words)\n",
    "                  and ((w.lower().isalnum()) or keep_punctuation)\n",
    "                  and (not (w.lower().isdigit()) or keep_numbers)]\n",
    "\n",
    "    if not keep_inflected:\n",
    "        normalized = lemmatize(normalized)\n",
    "\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLjVi37rpEgf"
   },
   "source": [
    "How does *keep_inflected* affect the output of __normalize__?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07wGhx8QpEgf",
    "outputId": "180a22e8-2870-4afb-a18e-fb83910df1f9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                        original                      normalized                        inflected\n",
      "0              The Limmat flows out of the lake.            [limmat, flow, lake]            [limmat, flows, lake]\n",
      "1  The bears are in the bear pit near the river.  [bear, bear, pit, near, river]  [bears, bear, pit, near, river]\n",
      "2            The Rhône flows out of Lake Geneva.     [rhône, flow, lake, geneva]     [rhône, flows, lake, geneva]\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "normalized_training_corpus = [normalize(item, keep_inflected=False) for item in training_corpus]    \n",
    "inflected_training_corpus = [normalize(item, keep_inflected=True) for item in training_corpus] \n",
    "\n",
    "df=pd.DataFrame(columns=['original', 'normalized', 'inflected'])\n",
    "df['original']=training_corpus\n",
    "df['normalized']=normalized_training_corpus\n",
    "df['inflected']=inflected_training_corpus\n",
    "print (df.to_string())\n",
    "# keep_inflected maintains inflected forms such as 'cities'\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM45JV-IpEgg"
   },
   "source": [
    "\n",
    "\n",
    "Now, we need to define a __get_vocabulary__ function that gets us all the unique words that appear in the normalized documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "Wt7xCA3UpEgh"
   },
   "outputs": [],
   "source": [
    "def get_vocabulary (data):\n",
    "    return list(set(sum(data,[])))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBG13TtHpEgh"
   },
   "source": [
    "Print the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9h0s9WspEgi",
    "outputId": "e4cf9343-3dc9-4ffe-c31f-80ccdc3ee44a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['river', 'pit', 'lake', 'flow', 'geneva', 'rhône', 'bear', 'limmat', 'near']\n",
      "Tokens in vocab:  9\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "print(get_vocabulary(normalized_training_corpus)) \n",
    "# END_REMOVE\n",
    "print ('Tokens in vocab: ', str(len(get_vocabulary(normalized_training_corpus))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbNiKvy1pEgk"
   },
   "source": [
    "We define a class __ms_timer__ that helps us time snippets of code. Its definition follows a special syntax that serves to implement what is known as a context manager. \n",
    "\n",
    "Each code snippet that we wish to time will be placed in an indented block following a __with__ statement. At the end of the indented block, the run time of the snippet will be returned by the class method __get_elapsed_time__. \n",
    "\n",
    "(You can do this same thing effortlessly in an IDE with profiling, but this is a good way to do it in a Jupyter notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "ZC-YXPdtpEgk"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class ms_timer:\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.stop = time.time()\n",
    "    \n",
    "    def get_elapsed_time(self):\n",
    "        return 1000 * (self.stop - self.start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyauDna5pEgl"
   },
   "source": [
    "Here's an example of how to time code snippets using the context manager trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X48pRwABpEgl",
    "outputId": "6e081215-c023-471e-c93f-3d2eba50856c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Elapsed time for the loop: 0.0145 ms\n"
     ]
    }
   ],
   "source": [
    "my_data = range(1, 10)\n",
    "\n",
    "with ms_timer() as timer:\n",
    "    prod=1\n",
    "    for item in my_data:\n",
    "        prod=prod*item\n",
    "print (\"Elapsed time for the loop: \"+str(round(timer.get_elapsed_time(), 4))+\" ms\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMXx3PfYpEgl"
   },
   "source": [
    "## MNB from scratch\n",
    "\n",
    "We are now ready to implement our MNB from scratch. Our implementation is contained in a class called __naive_bayes__. We can define our class across multiple cells simply by defining a derived class with exactly the same name in the following cells.\n",
    "\n",
    "First we compute the posterior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "83wWyD0ypEgm"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "    @staticmethod\n",
    "    def get_posterior_probabilities(training_data, verbose=True):\n",
    "        \"\"\"\n",
    "        This function calculates the posterior probabilities for each word in the vocabulary for each class in the\n",
    "        training set.\n",
    "\n",
    "        :param training_data: dict, contains the training documents and their labels\n",
    "        :param verbose: bool, whether to print debugging information\n",
    "        :return: dict, the posterior probabilities for each word in the vocabulary for each class in the training set\n",
    "        \"\"\"\n",
    "        posterior = {}\n",
    "        vocabulary = get_vocabulary(training_data['documents'])\n",
    "        lw = len(vocabulary)\n",
    "        classes = list(set(training_data['labels']))\n",
    "\n",
    "        for index, c in enumerate(classes):\n",
    "            tokens = sum(training_data['documents'][training_data['labels'] == c], [])\n",
    "            try:\n",
    "                den = len(tokens)\n",
    "            except:\n",
    "                den = 0\n",
    "\n",
    "            current_class_docs = tokens\n",
    "\n",
    "            for w in vocabulary:\n",
    "                num = current_class_docs.count(w)\n",
    "                posterior[(w, c)] = (1 + num) / (den + lw)\n",
    "\n",
    "                if verbose:\n",
    "                    print('_' * 30)\n",
    "                    message = 'Token ' + w + ' appears ' + str(num) + ' times in class ' + c\n",
    "                    message = re.sub('1 times', 'once', message)\n",
    "                    print(message)\n",
    "\n",
    "                    message = 'There are ' + str(den) + ' tokens in class ' + c\n",
    "                    message = re.sub('are 1 tokens', 'is 1 token', message)\n",
    "                    print(message)\n",
    "\n",
    "                    print(current_class_docs)\n",
    "                    print('Vocab size: ' + str(lw))\n",
    "                    print('Posterior without Laplace smoothing: ' + str(num) + '/' + str(den) + '=' + str(round(num / den, 2)))\n",
    "                    print('Posterior with Laplace smoothing: ' + str(1 + num) + '/' + str(den + lw) + '=' + str(round(posterior[(w, c)], 2)))\n",
    "\n",
    "        return posterior\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3kmwe63pEgn"
   },
   "source": [
    "3) What is the posterior probability of finding 'limmat' given that the document is tagged as 'zurich'? Complete the following code snippet to find out. Use *verbose* to see what's going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3cYtZlMbpEgn",
    "outputId": "35466f9f-6fd6-45ea-e341-9ce614bd5aaa"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.17\n"
     ]
    }
   ],
   "source": [
    "# The method get_posterior_probabilities expects the training data in the form of a data frame\n",
    "training_data = pd.DataFrame(columns=['documents', 'labels'])\n",
    "training_data['documents']=normalized_training_corpus\n",
    "training_data['labels']=training_labels\n",
    "\n",
    "# BEGIN_REMOVE\n",
    "posterior=NaiveBayes.get_posterior_probabilities(training_data, verbose=False)\n",
    "print (\"%.2f\"%posterior['limmat', 'zurich'])\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uprOWxgIpEgo"
   },
   "source": [
    "Complete the following code so we can train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "Ns6iAgEBpEgo"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes(NaiveBayes):\n",
    "    \n",
    "    def train(self, training_data, timing=False):\n",
    "            \n",
    "        classes = training_data['labels']\n",
    "\n",
    "        # BEGIN_REMOVE\n",
    "        with ms_timer() as timer:\n",
    "            P_c = [(training_data['labels'] == tagged_class).sum() / len(training_data) \n",
    "                   for tagged_class in classes]\n",
    "        if timing:\n",
    "            print('Priors probabilities computed in ' + \"%.2f\" % timer.get_elapsed_time() + \" ms\")\n",
    "        \n",
    "        with ms_timer() as timer:\n",
    "            posterior_p = self.get_posterior_probabilities(training_data, verbose=False)\n",
    "        if timing:\n",
    "            print('Posterior probabilities computed in ' + \"%.2f\" % timer.get_elapsed_time() + \" ms\")    \n",
    "        # END_REMOVE\n",
    "        \n",
    "        return P_c, posterior_p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YXrWDROpEgp"
   },
   "source": [
    "Now we get to train the classifier. \n",
    "\n",
    "Print out the prior probabilities and the posterior probabilities and answer the following questions:\n",
    "\n",
    "a) What is the lowest posterior probability that you observe and why?\n",
    "\n",
    "b) What is the highest posterior probability that you observe and why?\n",
    "\n",
    "c) Why are the prior probabilities all 1/3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTHtuCB4pEgp",
    "outputId": "d924fa46-a35b-4410-ca1f-f6523bc99cfa"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Priors probabilities computed in 2.37 ms\n",
      "Posterior probabilities computed in 2.02 ms\n",
      "Prior probabilities:\n",
      "['0.33', '0.33', '0.33']\n",
      "Posterior probabilities:\n",
      "      (token, class)  post_p\n",
      "0      (river, bern)    0.14\n",
      "1        (pit, bern)    0.14\n",
      "2       (lake, bern)    0.07\n",
      "3       (flow, bern)    0.07\n",
      "4     (geneva, bern)    0.07\n",
      "5      (rhône, bern)    0.07\n",
      "6       (bear, bern)    0.21\n",
      "7     (limmat, bern)    0.07\n",
      "8       (near, bern)    0.14\n",
      "9    (river, zurich)    0.08\n",
      "10     (pit, zurich)    0.08\n",
      "11    (lake, zurich)    0.17\n",
      "12    (flow, zurich)    0.17\n",
      "13  (geneva, zurich)    0.08\n",
      "14   (rhône, zurich)    0.08\n",
      "15    (bear, zurich)    0.08\n",
      "16  (limmat, zurich)    0.17\n",
      "17    (near, zurich)    0.08\n",
      "18   (river, geneva)    0.08\n",
      "19     (pit, geneva)    0.08\n",
      "20    (lake, geneva)    0.15\n",
      "21    (flow, geneva)    0.15\n",
      "22  (geneva, geneva)    0.15\n",
      "23   (rhône, geneva)    0.15\n",
      "24    (bear, geneva)    0.08\n",
      "25  (limmat, geneva)    0.08\n",
      "26    (near, geneva)    0.08\n",
      "______________________________\n",
      "Sorted in descending order:\n",
      "      (token, class)  post_p\n",
      "6       (bear, bern)    0.21\n",
      "16  (limmat, zurich)    0.17\n",
      "11    (lake, zurich)    0.17\n",
      "12    (flow, zurich)    0.17\n",
      "23   (rhône, geneva)    0.15\n",
      "22  (geneva, geneva)    0.15\n",
      "21    (flow, geneva)    0.15\n",
      "20    (lake, geneva)    0.15\n",
      "0      (river, bern)    0.14\n",
      "8       (near, bern)    0.14\n",
      "1        (pit, bern)    0.14\n",
      "25  (limmat, geneva)    0.08\n",
      "24    (bear, geneva)    0.08\n",
      "19     (pit, geneva)    0.08\n",
      "18   (river, geneva)    0.08\n",
      "17    (near, zurich)    0.08\n",
      "13  (geneva, zurich)    0.08\n",
      "15    (bear, zurich)    0.08\n",
      "14   (rhône, zurich)    0.08\n",
      "10     (pit, zurich)    0.08\n",
      "9    (river, zurich)    0.08\n",
      "26    (near, geneva)    0.08\n",
      "7     (limmat, bern)    0.07\n",
      "5      (rhône, bern)    0.07\n",
      "4     (geneva, bern)    0.07\n",
      "3       (flow, bern)    0.07\n",
      "2       (lake, bern)    0.07\n"
     ]
    }
   ],
   "source": [
    "nb=NaiveBayes()\n",
    "P_c, posterior_p=nb.train(training_data, timing=True) \n",
    "\n",
    "# BEGIN_REMOVE\n",
    "print ('Prior probabilities:')\n",
    "print ([str(round(x, 2)) for x in P_c])\n",
    "\n",
    "print ('Posterior probabilities:')\n",
    "df=pd.DataFrame()\n",
    "df['(token, class)']=[x for x in posterior_p.keys()]\n",
    "df['post_p']=list(map(lambda x:round(x, 2), posterior_p.values()))\n",
    "print (df.to_string())\n",
    "print ('_'*30)\n",
    "print ('Sorted in descending order:')\n",
    "print (df.sort_values(by='post_p', ascending=False).to_string())\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6g453FSpEgp"
   },
   "source": [
    "And we get to do the classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "RQ_Ex1aGpEgq"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes(NaiveBayes):\n",
    "    \n",
    "    def classify_document(self, training_data, test_document, verbose=False):\n",
    "        from functools import reduce\n",
    "        import math\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        classes = list(set(training_data['labels']))\n",
    "\n",
    "        P_c, posterior_p = self.train(training_data)\n",
    "\n",
    "        NB = dict()\n",
    "\n",
    "        normalized_test_document = normalize(test_document, keep_inflected=False)\n",
    "\n",
    "        if verbose:\n",
    "            print('_' * 30)\n",
    "            print('Test doc: ', test_document)\n",
    "            print('Normalized test doc: ', normalized_test_document)\n",
    "\n",
    "        for index, c in enumerate(classes):\n",
    "\n",
    "            if verbose:\n",
    "              print('Class: ', c)\n",
    "\n",
    "            posterior_logsum = 0\n",
    "\n",
    "            for token in normalized_test_document:\n",
    "\n",
    "                if verbose:\n",
    "                    print('Token: ', token)\n",
    "\n",
    "                try:\n",
    "                    posterior_logsum = posterior_logsum + math.log(posterior_p[token, c], 10)\n",
    "\n",
    "                    if verbose:\n",
    "                        print('Posterior (' + token + ', ' + c + '): ' + str(posterior_p[token, c]))\n",
    "                        print('Posterior logsum: ', posterior_logsum)\n",
    "\n",
    "\n",
    "                except:\n",
    "                    if verbose:\n",
    "                        print('Token not in training ')\n",
    "\n",
    "            if posterior_logsum == 0:\n",
    "                print('Classification failure: insufficient info')\n",
    "\n",
    "            NB[c] = round(posterior_logsum + math.log(P_c[index], 10), 2)\n",
    "\n",
    "            if verbose:\n",
    "                print('Class: ', c)\n",
    "                print('NB: ', NB[c])\n",
    "\n",
    "        return max(NB, key=NB.get), NB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test our classifier with a simple sentence and see what happens."
   ],
   "metadata": {
    "id": "IaqjODGtOuXk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BEGIN_REMOVE\n",
    "\n",
    "import logging\n",
    "nb=NaiveBayes()\n",
    "test_corpus = \"On my way to the lake, I fell asleep near the Limmat.\"\n",
    "test_labels = \"zurich\"\n",
    "\n",
    "print (test_corpus)\n",
    "\n",
    "with ms_timer() as timer:\n",
    "    result, NB = nb.classify_document(training_data, test_corpus, verbose=True)\n",
    "    \n",
    "logging.warning('Classification completed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "print ('Classification result: ', result)   \n",
    "print (NB)\n",
    "\n",
    "# END_REMOVE"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioDKnsX2MUc0",
    "outputId": "aa156622-8f85-4bf9-c9cf-ce6a9b7fff24"
   },
   "execution_count": 97,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Classification completed in 7.39 ms\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "On my way to the lake, I fell asleep near the Limmat.\n",
      "______________________________\n",
      "Test doc:  On my way to the lake, I fell asleep near the Limmat.\n",
      "Normalized test doc:  ['way', 'lake', 'fell', 'asleep', 'near', 'limmat']\n",
      "Class:  bern\n",
      "Token:  way\n",
      "Token not in training \n",
      "Token:  lake\n",
      "Posterior (lake, bern): 0.07142857142857142\n",
      "Posterior logsum:  -1.146128035678238\n",
      "Token:  fell\n",
      "Token not in training \n",
      "Token:  asleep\n",
      "Token not in training \n",
      "Token:  near\n",
      "Posterior (near, bern): 0.14285714285714285\n",
      "Posterior logsum:  -1.9912260756924947\n",
      "Token:  limmat\n",
      "Posterior (limmat, bern): 0.07142857142857142\n",
      "Posterior logsum:  -3.1373541113707324\n",
      "Class:  bern\n",
      "NB:  -3.61\n",
      "Class:  zurich\n",
      "Token:  way\n",
      "Token not in training \n",
      "Token:  lake\n",
      "Posterior (lake, zurich): 0.16666666666666666\n",
      "Posterior logsum:  -0.7781512503836435\n",
      "Token:  fell\n",
      "Token not in training \n",
      "Token:  asleep\n",
      "Token not in training \n",
      "Token:  near\n",
      "Posterior (near, zurich): 0.08333333333333333\n",
      "Posterior logsum:  -1.857332496431268\n",
      "Token:  limmat\n",
      "Posterior (limmat, zurich): 0.16666666666666666\n",
      "Posterior logsum:  -2.6354837468149115\n",
      "Class:  zurich\n",
      "NB:  -3.11\n",
      "Class:  geneva\n",
      "Token:  way\n",
      "Token not in training \n",
      "Token:  lake\n",
      "Posterior (lake, geneva): 0.15384615384615385\n",
      "Posterior logsum:  -0.8129133566428555\n",
      "Token:  fell\n",
      "Token not in training \n",
      "Token:  asleep\n",
      "Token not in training \n",
      "Token:  near\n",
      "Posterior (near, geneva): 0.07692307692307693\n",
      "Posterior logsum:  -1.9268567089496922\n",
      "Token:  limmat\n",
      "Posterior (limmat, geneva): 0.07692307692307693\n",
      "Posterior logsum:  -3.0408000612565287\n",
      "Class:  geneva\n",
      "NB:  -3.52\n",
      "Classification result:  zurich\n",
      "{'bern': -3.61, 'zurich': -3.11, 'geneva': -3.52}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-D8LtPT9pEgq"
   },
   "source": [
    "Test your classifier with the test document *The name of the city comes from the word 'bear'.* What goes wrong? Can you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjtW7IlCpEgr",
    "outputId": "98dd56e1-c073-4c02-f53b-f7ba4869254c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The name of the city comes from the word 'bear'\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Classification completed in 14.31 ms\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "______________________________\n",
      "Test doc:  The name of the city comes from the word 'bear'\n",
      "Normalized test doc:  ['name', 'city', 'come', 'word']\n",
      "Class:  bern\n",
      "Token:  name\n",
      "Token not in training \n",
      "Token:  city\n",
      "Token not in training \n",
      "Token:  come\n",
      "Token not in training \n",
      "Token:  word\n",
      "Token not in training \n",
      "Classification failure: insufficient info\n",
      "Class:  bern\n",
      "NB:  -0.48\n",
      "Class:  zurich\n",
      "Token:  name\n",
      "Token not in training \n",
      "Token:  city\n",
      "Token not in training \n",
      "Token:  come\n",
      "Token not in training \n",
      "Token:  word\n",
      "Token not in training \n",
      "Classification failure: insufficient info\n",
      "Class:  zurich\n",
      "NB:  -0.48\n",
      "Class:  geneva\n",
      "Token:  name\n",
      "Token not in training \n",
      "Token:  city\n",
      "Token not in training \n",
      "Token:  come\n",
      "Token not in training \n",
      "Token:  word\n",
      "Token not in training \n",
      "Classification failure: insufficient info\n",
      "Class:  geneva\n",
      "NB:  -0.48\n",
      "bern\n",
      "{'bern': -0.48, 'zurich': -0.48, 'geneva': -0.48}\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "\n",
    "import logging\n",
    "nb=NaiveBayes()\n",
    "test_corpus = \"The name of the city comes from the word 'bear'\"\n",
    "test_labels = \"bern\"\n",
    "\n",
    "print (test_corpus)\n",
    "\n",
    "with ms_timer() as timer:\n",
    "    result, NB = nb.classify_document(training_data, test_corpus, verbose=True)\n",
    "    \n",
    "logging.warning('Classification completed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "print (result)   \n",
    "print (NB)\n",
    "\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5DyNdzcpEgr",
    "outputId": "74d529e8-0045-4d01-b5cb-97c1acee8e02"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The name of the city comes from the word bear\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Classification completed in 4.28 ms\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification result:  ('bern', {'bern': -1.15, 'zurich': -1.56, 'geneva': -1.59})\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "\n",
    "import logging\n",
    "nb=NaiveBayes()\n",
    "test_corpus = \"The name of the city comes from the word 'bear'\"\n",
    "test_labels = \"bern\"\n",
    "\n",
    "test_corpus=test_corpus.replace('\\'', '')\n",
    "\n",
    "print (test_corpus)\n",
    "\n",
    "with ms_timer() as timer:\n",
    "    result = nb.classify_document(training_data, test_corpus, verbose=False)\n",
    "    \n",
    "logging.warning('Classification completed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "print ('Classification result: ', result)  \n",
    "\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_3NAGDhpEgr"
   },
   "source": [
    "Can you explain the performance of your classifier on the following test corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bkPW2M-RpEgr",
    "outputId": "3c989010-fea1-42d8-9e3d-33bffd360cd7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Classification of \"We saw the bears there.\" completed in 10.60 ms\n",
      "WARNING:root:Classification of \"We crossed the Rhône.\" completed in 3.76 ms\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Classifying: We saw the bears there.\n",
      "('bern', {'bern': -1.15, 'zurich': -1.56, 'geneva': -1.59})\n",
      "correct label: bern\n",
      "\n",
      " Classifying: We crossed the Rhône.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Classification of \"There is no lake.\" completed in 5.51 ms\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('geneva', {'bern': -1.62, 'zurich': -1.56, 'geneva': -1.29})\n",
      "correct label: geneva\n",
      "\n",
      " Classifying: There is no lake.\n",
      "('zurich', {'bern': -1.62, 'zurich': -1.26, 'geneva': -1.29})\n",
      "correct label: bern\n"
     ]
    }
   ],
   "source": [
    "test_corpus = ['We saw the bears there.', \n",
    "               'We crossed the Rhône.', \n",
    "               'There is no lake.',\n",
    "              ]\n",
    "test_labels = ['bern',\n",
    "               'geneva',\n",
    "               'bern',\n",
    "              ]\n",
    "\n",
    "nb=NaiveBayes() \n",
    "\n",
    "\n",
    "\n",
    "for item in test_corpus:\n",
    "    print ('\\n Classifying: '+item)\n",
    "    with ms_timer() as timer:\n",
    "        result = nb.classify_document(training_data, item)\n",
    "    logging.warning('Classification of \\\"'+item+'\\\" completed in '+\"%.2f\"%timer.get_elapsed_time()+\" ms\")    \n",
    "    print (result)                                      \n",
    "    print ('correct label: '+test_labels[test_corpus.index(item)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veW4nLE6pEgr"
   },
   "source": [
    "Now test your classifier with the one-sentence document \"The federal capital is pretty.\" What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aWCu98F5pEgt",
    "outputId": "9dc61746-da4c-4a4c-f81c-8d78917689c9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classification failure: insufficient info\n",
      "Classification failure: insufficient info\n",
      "Classification failure: insufficient info\n",
      "('bern', {'bern': -0.48, 'zurich': -0.48, 'geneva': -0.48})\n"
     ]
    }
   ],
   "source": [
    "# BEGIN_REMOVE\n",
    "test_corpus = \"The federal capital is pretty.\"\n",
    "test_labels = \"bern\"\n",
    "\n",
    "# Your classifier fails because your test document contains a previously unseen word. \n",
    "print (nb.classify_document(training_data, test_corpus))\n",
    "# END_REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_CMS5VypEgt",
    "outputId": "87463ac1-adbe-4871-9042-e21891c22816"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('zurich', {'bern': -1.62, 'zurich': -1.26, 'geneva': -1.29})\n"
     ]
    }
   ],
   "source": [
    "test_corpus = \"There is no lake.\"\n",
    "test_labels = \"zurich\"\n",
    "\n",
    "print (nb.classify_document(training_data, test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "MEP question!!!!!!!!!!!!!!¨"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        rigi       1.00      0.50      0.67         2\n",
      "     pilatus       0.50      1.00      0.67         1\n",
      "        dole       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.83      0.83      0.78         4\n",
      "weighted avg       0.88      0.75      0.75         4\n"
     ]
    }
   ],
   "source": [
    "test_corpus = ['It’s surrounded by three different lakes.', \n",
    "               'You can hike to the top from Goldau.', \n",
    "               'It’s across the lake from Rigi.',\n",
    "               'Meteoschweiz has a radar at the top.',\n",
    "              ]\n",
    "test_labels = ['rigi',\n",
    "               'rigi',\n",
    "               'pilatus',\n",
    "               'dole',\n",
    "              ]\n",
    "\n",
    "predicted_labels = ['pilatus',\n",
    "               'rigi',\n",
    "               'pilatus',\n",
    "               'dole',\n",
    "              ]\n",
    "\n",
    "target_names = sorted(list(set(test_labels)))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['rigi', 'pilatus', 'dole']\n",
    "print(classification_report(test_labels, predicted_labels, labels = target_names, target_names=target_names))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdtMb419pEgu",
    "outputId": "81e59eb4-dbee-443c-9d3b-58ce753b7bb8"
   },
   "execution_count": 103
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
